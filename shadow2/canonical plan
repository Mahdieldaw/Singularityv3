You're right - that response had **two documents**:

1. **"Consolidated Implementation Plan v1.1 Final"** - The FULL canonical plan with all corrections integrated (~8000 words)
2. **"Implementation Guidance for AI Agent"** - A SHORT natural language prompt (<6000 chars) to accompany it

Let me clarify what you should hand your agent:

---

## What to Give Your Agent

### Primary Document: The Full Plan
Give him the **"Consolidated Implementation Plan: Embedding-Based Paragraph Clustering (v1.1 Final)"** section from my response above. This is the complete canonical plan with all 5 critical corrections integrated. It includes:

- All 8 phases with full code
- Complete file manifest
- All type definitions
- Integration details
- Implementation order

This is **standalone and complete** - it has everything from the original canonical plan PLUS the corrections.

### Secondary Document: The Natural Language Guide
The **"Implementation Guidance for AI Agent"** section is a **companion prompt** that:
- Summarizes the mission in plain English
- Highlights the 5 critical corrections upfront
- Lists common pitfalls to avoid
- Provides a quick-start checklist

This is meant to be read ALONGSIDE the full plan, not instead of it.

---

## Recommended Handoff

Give your agent this prompt:

```
You are implementing an embedding-based semantic clustering system. 

Read these two documents:

1. "Consolidated Implementation Plan v1.1 Final" - Your complete specification. 
   Follow it exactly. Every code block, every type definition, every phase.

2. "Implementation Guidance for AI Agent" - Highlights the 5 critical corrections 
   and common pitfalls. Read this FIRST to understand what changed from earlier 
   drafts, then implement according to the full plan.

Key corrections applied:
- Quantized similarity for determinism
- Stable tie-breakers in HAC
- Max clusters safety limit (40)
- Embedding input uses original ShadowStatement texts (not _fullParagraph)
- Parser enforcement as Phase 0

Start with Phase 0 (parser validation), then proceed through phases 1-5.
The full plan has complete implementations for every file.
```

---

## To Directly Answer Your Question

**Is the guide standalone?** 

- The **"Implementation Guidance"** alone: **No** - it's a summary/orientation doc
- The **"v1.1 Final Plan"** alone: **Yes** - it's the full canonical plan with corrections

**Is it shorter?**

- The guidance section is intentionally short (~3000 chars) to fit context limits
- The full v1.1 plan is the same length as the original canonical plan (~8000 words)

---

Would you like me to:
1. ✅ Confirm the v1.1 plan above is complete and standalone (it is)
2. ✅ Provide a single merged document that combines both
3. ✅ Create a different format for your agent

The v1.1 plan I provided is your canonical spec with all corrections integrated. It's ready to hand off as-is.



# Consolidated Implementation Plan: Embedding-Based Paragraph Clustering (v1.1 Final)

**Version:** 1.1 Final with Critical Corrections  
**Status:** Ready for Implementation  
**Estimated Scope:** 12 new files, 6 modified files

---

## Critical Corrections from Review

### 1. Quantized Similarity for Determinism
**Added to `src/clustering/distance.ts`:**
```typescript
export function quantizeSimilarity(sim: number): number {
    return Math.round(sim * 1e6) / 1e6;
}
```

Use this in all similarity comparisons to prevent floating-point drift.

### 2. Stable Tie-Breakers in HAC
**Added to `src/clustering/hac.ts`:**
```typescript
// When multiple merges have equal similarity
if (dist < minDist || (dist === minDist && i < j)) {
    minDist = dist;
    minI = i;
    minJ = j;
}
```

### 3. Max Clusters Safety Limit
**Added to `src/clustering/config.ts`:**
```typescript
export const DEFAULT_CONFIG: ClusteringConfig = {
    similarityThreshold: 0.82,
    maxClusters: 40,  // NEW: Safety limit
    // ... rest
};
```

**Added to HAC algorithm:**
```typescript
// Stop if under limit and threshold exceeded
if (active.size <= config.maxClusters && minDist > distanceThreshold) {
    break;
}
// Force merge if over limit (prevent explosion)
if (active.size > config.maxClusters) {
    // Merge closest pair even if over threshold
}
```

### 4. Embedding Input Text (CRITICAL FIX)
**Changed from:**
```typescript
const texts = paragraphs.map(p => p._fullParagraph || p.statements.map(s => s.text).join(' '));
```

**To:**
```typescript
// Build from original ShadowStatement texts (unclipped), NOT prompt-clipped paragraph.statements
export async function generateEmbeddings(
    paragraphs: ShadowParagraph[],
    shadowStatements: ShadowStatement[],  // NEW: needed for unclipped text
    config: ClusteringConfig = DEFAULT_CONFIG
): Promise<EmbeddingResult>
```

**Implementation:**
```typescript
const statementsById = new Map(shadowStatements.map(s => [s.id, s]));
const texts = paragraphs.map(p => 
    p.statementIds
        .map(sid => statementsById.get(sid)?.text || '')
        .join(' ')
);
```

**Rationale:** 
- Embeddings must use extracted evidence (same as what clustering operates on)
- `_fullParagraph` reserved for expansion text only
- Prompt-clipped text (320 chars) would lose semantic information

### 5. Parser Enforcement (NEW PHASE 0)
**Add to `shared/parsing-utils.ts` or `semanticMapper.ts`:**

```typescript
export function validateProvenance(
    claims: SemanticClaim[],
    validStatementIds: Set<string>
): ParseError[] {
    const errors: ParseError[] = [];
    
    for (const claim of claims) {
        for (const id of claim.sourceStatementIds) {
            // HARD ERROR: paragraph/cluster IDs not allowed
            if (id.startsWith('p_') || id.startsWith('pc_')) {
                errors.push({
                    field: 'sourceStatementIds',
                    issue: `Invalid ID: ${id}. Only s_* IDs allowed (paragraph/cluster IDs are not citable).`,
                    severity: 'error',
                });
            }
            // STRICT: unknown statement IDs
            else if (!validStatementIds.has(id)) {
                errors.push({
                    field: 'sourceStatementIds',
                    issue: `Unknown statement ID: ${id}`,
                    severity: 'error',
                });
            }
        }
        
        // Check gates and conflicts too
        for (const gate of [...(claim.gates?.conditionals || []), ...(claim.gates?.prerequisites || [])]) {
            for (const id of gate.sourceStatementIds || []) {
                if (id.startsWith('p_') || id.startsWith('pc_')) {
                    errors.push({ severity: 'error', issue: `Gate cites invalid ID: ${id}` });
                }
            }
        }
        
        for (const conflict of claim.conflicts || []) {
            for (const id of conflict.sourceStatementIds || []) {
                if (id.startsWith('p_') || id.startsWith('pc_')) {
                    errors.push({ severity: 'error', issue: `Conflict cites invalid ID: ${id}` });
                }
            }
        }
    }
    
    return errors;
}
```

---

## Phase 2: Offscreen Embeddings (Implementation Details)

### Integration with Existing Bus System

**Your codebase already has:**
- Offscreen document: Created from `sw-entry.js`
- Message bus: `BusController.js` (JSON-based serialization)
- Event handlers: `OffscreenBootstrap.js`

**Key Gap:** Bus uses `JSON.stringify`/`JSON.parse`, which doesn't preserve `Float32Array`.

**Solution:**
```typescript
// Offscreen returns number[][] over the bus
// Service worker rehydrates to Float32Array[]

// In offscreen handler:
return {
    embeddings: allEmbeddings.map(f32 => Array.from(f32)),  // number[][]
    dimensions: targetDimensions,
    timeMs: performance.now() - startTime,
};

// In service worker embeddings.ts:
const embeddings = new Map<string, Float32Array>();
for (let i = 0; i < ids.length; i++) {
    embeddings.set(ids[i], Float32Array.from(response.result.embeddings[i]));
}
```

### Message Contract

**Register in offscreen:**
```typescript
// In OffscreenBootstrap.js or new embeddings handler
bus.register('embeddings.embedTexts', async (texts: string[], opts: { 
    dims: number; 
    modelId: string; 
    mode?: 'wasm' | 'webgpu' | 'auto' 
}) => {
    // Returns: { embeddings: number[][], dimensions: number, timeMs: number }
});

bus.register('embeddings.ping', async () => {
    return { ready: !!embedder };
});

bus.register('embeddings.status', async () => {
    return { 
        ready: !!embedder, 
        backend: currentBackend,
        modelId: currentModelId 
    };
});
```

### Backend Selection
```typescript
// In offscreen embedding-worker.ts
const hasWebGPU = !!navigator.gpu;
const backend = opts.mode === 'wasm' ? 'wasm' 
              : opts.mode === 'webgpu' ? 'webgpu'
              : hasWebGPU ? 'webgpu'  // auto: prefer WebGPU
              : 'wasm';
```

### Runtime Choice: @xenova/transformers (Recommended)

**Add to `package.json`:**
```json
{
    "dependencies": {
        "@xenova/transformers": "^2.17.0"
    }
}
```

**Why:** Handles WebGPU/WASM fallback + tokenization + model loading out-of-box.

### Model Bundling in Your Build System

**Your build flow:**
1. `esbuild` compiles JS
2. `scripts/postbuild.js` copies static files

**Add to `scripts/postbuild.js`:**
```javascript
// Copy model artifacts
fs.cpSync(
    path.join(__dirname, '../models'),
    path.join(__dirname, '../dist/models'),
    { recursive: true }
);
```

**Add to `manifest.json`:**
```json
{
    "web_accessible_resources": [{
        "resources": ["models/*"],
        "matches": ["<all_urls>"]
    }]
}
```

**CSP is already set:** `wasm-unsafe-eval` present for WASM backends.

### Determinism Enforcement (Service Worker Side)

**In `src/clustering/embeddings.ts`:**
```typescript
function normalizeEmbedding(vec: Float32Array): Float32Array {
    let norm = 0;
    for (let i = 0; i < vec.length; i++) {
        norm += vec[i] * vec[i];
    }
    norm = Math.sqrt(norm);
    
    if (norm > 0) {
        for (let i = 0; i < vec.length; i++) {
            vec[i] /= norm;
        }
    }
    
    return vec;
}

// After receiving from offscreen:
for (let i = 0; i < ids.length; i++) {
    let emb = Float32Array.from(response.result.embeddings[i]);
    
    // Truncate if needed (MRL)
    if (emb.length > config.embeddingDimensions) {
        emb = emb.slice(0, config.embeddingDimensions);
    }
    
    // Renormalize after truncation
    emb = normalizeEmbedding(emb);
    
    embeddings.set(ids[i], emb);
}
```

### Concurrency + Caching

**In offscreen worker:**
```typescript
let modelCache: Map<string, Pipeline> = new Map();
let inFlightQueue: Promise<any> | null = null;

async function ensureModel(modelId: string, backend: string): Promise<Pipeline> {
    const cacheKey = `${modelId}:${backend}`;
    if (modelCache.has(cacheKey)) {
        return modelCache.get(cacheKey)!;
    }
    
    // Single-flight: wait for any in-progress load
    if (inFlightQueue) {
        await inFlightQueue;
    }
    
    inFlightQueue = (async () => {
        const model = await pipeline('feature-extraction', modelId, {
            device: backend,
            dtype: 'q8',
        });
        modelCache.set(cacheKey, model);
        return model;
    })();
    
    return inFlightQueue;
}
```

---

## Phase 3: Clustering Module (Implementation Details)

### Critical API Fix: Accept Original Statements

**OLD (wrong):**
```typescript
export async function clusterParagraphs(
    paragraphs: ShadowParagraph[],
    config: Partial<ClusteringConfig> = {}
): Promise<ClusteringResult>
```

**NEW (correct):**
```typescript
export async function clusterParagraphs(
    paragraphs: ShadowParagraph[],
    shadowStatements: ShadowStatement[],  // NEW: needed for unclipped text
    config: Partial<ClusteringConfig> = {}
): Promise<ClusteringResult>
```

### HAC Determinism Implementation

**In `src/clustering/hac.ts`:**

```typescript
export function hierarchicalCluster(
    paragraphIds: string[],
    distances: number[][],
    config: ClusteringConfig
): number[][] {
    const n = paragraphIds.length;
    
    // Initialize clusters with stable IDs
    const clusters: Set<number>[] = Array.from({ length: n }, (_, i) => new Set([i]));
    const active = new Set(Array.from({ length: n }, (_, i) => i));
    
    const distanceThreshold = 1 - config.similarityThreshold;
    
    while (active.size > 1) {
        // Find closest pair
        let minDist = Infinity;
        let minI = -1;
        let minJ = -1;
        
        const activeArray = Array.from(active).sort((a, b) => a - b);  // Stable order
        
        for (let ai = 0; ai < activeArray.length; ai++) {
            for (let aj = ai + 1; aj < activeArray.length; aj++) {
                const i = activeArray[ai];
                const j = activeArray[aj];
                const dist = quantizeSimilarity(averageLinkage(clusters[i], clusters[j], distances));
                
                // Tie-breaker: prefer lower index pairs
                if (dist < minDist || (dist === minDist && i < j)) {
                    minDist = dist;
                    minI = i;
                    minJ = j;
                }
            }
        }
        
        // Stop conditions
        if (active.size <= config.maxClusters && minDist > distanceThreshold) {
            break;  // Under limit and threshold exceeded
        }
        
        if (active.size > config.maxClusters) {
            // Force merge to prevent explosion
            // (minDist already found above)
        }
        
        // Merge j into i
        for (const idx of clusters[minJ]) {
            clusters[minI].add(idx);
        }
        active.delete(minJ);
    }
    
    // Convert to stable output
    return Array.from(active)
        .sort((a, b) => a - b)
        .map(i => Array.from(clusters[i]).sort((a, b) => a - b));
}
```

### Centroid Selection with Tie-Breaking

**In `src/clustering/engine.ts`:**

```typescript
function findCentroid(
    memberIds: string[],
    embeddings: Map<string, Float32Array>
): { id: string; similarity: number } {
    if (memberIds.length === 1) {
        return { id: memberIds[0], similarity: 1.0 };
    }
    
    // Compute mean
    const dim = embeddings.get(memberIds[0])!.length;
    const mean = new Float32Array(dim);
    
    for (const id of memberIds) {
        const emb = embeddings.get(id)!;
        for (let i = 0; i < dim; i++) {
            mean[i] += emb[i];
        }
    }
    
    // Normalize mean
    let norm = 0;
    for (let i = 0; i < dim; i++) {
        mean[i] /= memberIds.length;
        norm += mean[i] * mean[i];
    }
    norm = Math.sqrt(norm);
    if (norm > 0) {
        for (let i = 0; i < dim; i++) {
            mean[i] /= norm;
        }
    }
    
    // Find closest member (tie-break by lexicographic ID)
    let bestId = memberIds[0];
    let bestSim = -Infinity;
    
    for (const id of memberIds) {
        const emb = embeddings.get(id)!;
        let sim = 0;
        for (let i = 0; i < dim; i++) {
            sim += emb[i] * mean[i];
        }
        const simQ = quantizeSimilarity(sim);
        
        // Tie-breaker: lexicographically smallest ID
        if (simQ > bestSim || (simQ === bestSim && id < bestId)) {
            bestSim = simQ;
            bestId = id;
        }
    }
    
    return { id: bestId, similarity: bestSim };
}
```

### Uncertainty Detection (Complete)

**In `src/clustering/engine.ts`:**

```typescript
function detectUncertainty(
    paragraphIds: string[],
    paragraphsById: Map<string, ShadowParagraph>,
    cohesion: number,
    config: ClusteringConfig
): { uncertain: boolean; reasons: string[] } {
    const reasons: string[] = [];
    
    // Check in fixed order for determinism
    if (cohesion < config.lowCohesionThreshold) {
        reasons.push('low_cohesion');
    }
    
    if (paragraphIds.length > config.maxClusterSize) {
        reasons.push('oversized');
    }
    
    // Stance diversity
    const uniqueStances = new Set<Stance>();
    for (const pid of paragraphIds) {
        const p = paragraphsById.get(pid);
        if (p) uniqueStances.add(p.dominantStance);
    }
    if (uniqueStances.size >= config.stanceDiversityThreshold) {
        reasons.push('stance_diversity');
    }
    
    // Contested ratio
    let contestedCount = 0;
    for (const pid of paragraphIds) {
        if (paragraphsById.get(pid)?.contested) contestedCount++;
    }
    if (contestedCount / paragraphIds.length > config.contestedRatioThreshold) {
        reasons.push('high_contested_ratio');
    }
    
    // Conflicting signals
    let hasTension = false;
    let hasConditional = false;
    for (const pid of paragraphIds) {
        const p = paragraphsById.get(pid);
        if (p?.signals.tension) hasTension = true;
        if (p?.signals.conditional) hasConditional = true;
    }
    if (hasTension && hasConditional && paragraphIds.length > 1) {
        reasons.push('conflicting_signals');
    }
    
    return { uncertain: reasons.length > 0, reasons };
}
```

### Expansion Selection (Deterministic)

**In `src/clustering/engine.ts`:**

```typescript
function buildExpansion(
    paragraphIds: string[],
    centroidId: string,
    paragraphsById: Map<string, ShadowParagraph>,
    embeddings: Map<string, Float32Array>,
    config: ClusteringConfig
): ParagraphCluster['expansion'] {
    const centroidEmb = embeddings.get(centroidId)!;
    
    // Find most distant member
    const memberSims = paragraphIds
        .map(pid => ({
            pid,
            sim: quantizeSimilarity(cosineSimilarity(embeddings.get(pid)!, centroidEmb))
        }))
        .sort((a, b) => a.sim - b.sim || a.pid.localeCompare(b.pid));  // Tie-break by ID
    
    // Select: centroid + most distant + fill to limit
    const selected = new Set<string>([centroidId]);
    for (const { pid } of memberSims) {
        if (selected.size >= config.maxExpansionMembers) break;
        selected.add(pid);
    }
    
    // Build expansion with char budget
    const members: Array<{ paragraphId: string; text: string }> = [];
    let charBudget = config.maxExpansionCharsTotal;
    
    // Centroid first, then by similarity order
    for (const pid of [centroidId, ...memberSims.map(m => m.pid)]) {
        if (!selected.has(pid)) continue;
        selected.delete(pid);
        
        const p = paragraphsById.get(pid);
        if (!p) continue;
        
        const text = clipText(p._fullParagraph || '', config.maxMemberTextChars);
        if (charBudget - text.length < 0) break;
        
        charBudget -= text.length;
        members.push({ paragraphId: pid, text });
    }
    
    return { members };
}
```

### Representative Text (IMPORTANT)

**Do NOT include `representativeText` field in cluster object.** Only `representativeParagraphId`.

**Rationale:** Semantic mapper should look up the representative paragraph and use its statements directly, avoiding any text synthesis or duplication.

---

## Updated File Manifest with Corrections

### New Files
1. `src/clustering/types.ts` - Add `maxClusters` to config interface
2. `src/clustering/config.ts` - Add `maxClusters: 40`
3. `src/clustering/distance.ts` - Add `quantizeSimilarity()` function
4. `src/clustering/hac.ts` - Implement stable tie-breakers + max cluster handling
5. `src/clustering/engine.ts` - Accept `shadowStatements` parameter, use unclipped text
6. `src/clustering/embeddings.ts` - Accept `shadowStatements`, rehydrate Float32Array, renormalize
7. `src/clustering/index.ts` - Update signatures
8. `src/offscreen/embedding-worker.ts` - Return `number[][]`, implement caching
9. `src/shadow/ShadowParagraphProjector.ts` - As specified
10. `scripts/postbuild.js` - Add model copy step

### Modified Files
1. `src/shadow/index.ts` - Export projector
2. `src/ConciergeService/semanticMapper.ts` - Remove duplication, uncertain-first ordering, **remove representativeText usage**
3. `StepExecutor.js` - Pass `shadowStatements` to clustering
4. `shared/parsing-utils.ts` - Add `validateProvenance()` (Phase 0)
5. `manifest.json` - Verify `web_accessible_resources` for models
6. `package.json` - Add `@xenova/transformers`

---

## Implementation Order

```
Phase 0 (Day 1): Parser Enforcement
├── Add validateProvenance() to parsing-utils.ts
└── Wire into semanticMapper parsing flow

Phase 1 (Days 2-3): Shadow Fixes + Projector
├── Modify ShadowExtractor (markdown, tables)
├── Create ShadowParagraphProjector
└── Unit tests

Phase 2 (Days 4-6): Embeddings
├── Add @xenova/transformers dependency
├── Create embedding-worker.ts in offscreen
├── Create embeddings.ts service wrapper
├── Add model bundling to postbuild
└── Integration tests

Phase 3 (Days 7-10): Clustering
├── types.ts, config.ts (with maxClusters)
├── distance.ts (with quantizeSimilarity)
├── hac.ts (with tie-breakers)
├── engine.ts (with shadowStatements param)
├── index.ts
└── Unit tests

Phase 4 (Days 11-12): Integration
├── StepExecutor changes
├── semanticMapper changes
└── End-to-end tests

Phase 5 (Days 13-14): Polish
├── Edge case handling
├── Performance tuning
└── Documentation
```

---

## Summary of Critical Corrections

1. **Quantized similarity** prevents non-determinism
2. **Stable tie-breakers** in HAC ensure same input → same output
3. **Max clusters (40)** prevents pathological explosion
4. **Embedding input** uses original statement texts (unclipped), not `_fullParagraph`
5. **Parser enforcement** blocks `p_*` / `pc_*` citations as hard errors

With these fixes, the plan is **ready for implementation**.

---

# Implementation Guidance for AI Agent (Under 6000 chars)

You're building an **embedding-based semantic clustering layer** between Shadow extraction and Semantic Mapping. This replaces Jaccard (vocabulary-based) with cosine similarity (meaning-based).

## Your Mission

Implement a system that:
1. Projects Shadow statements onto paragraphs (with contested detection)
2. Generates embeddings for paragraphs using WebGPU/WASM
3. Clusters paragraphs hierarchically using similarity threshold 0.82
4. Detects uncertain clusters and provides expansion text for review
5. Integrates with existing StepExecutor → SemanticMapper flow

## Critical Requirements (Non-Negotiable)

### Determinism
- **Quantize all similarities:** `Math.round(sim * 1e6) / 1e6`
- **Stable tie-breakers:** When similarities equal, choose lexicographically smallest IDs
- **Renormalize after truncation:** L2 norm → truncate → L2 norm again

### Embedding Input Text
- Build from **original ShadowStatement.text** (unclipped), joined by `paragraph.statementIds` order
- **NOT** from `paragraph._fullParagraph` (reserved for expansion)
- **NOT** from `paragraph.statements[].text` (prompt-clipped to 320 chars)

### Provenance
- **Phase 0 FIRST:** Add parser validation that **errors** (not warns) on:
  - Any `p_*` or `pc_*` IDs in `sourceStatementIds`
  - Any unknown `s_*` IDs
- This prevents hallucinated citations from being accepted as valid

### Centroid Selection
- Representative = paragraph closest to cluster mean embedding
- Tie-break by lexicographically smallest paragraph ID
- **Do not** include `representativeText` field in cluster object (semantic mapper looks it up)

### Max Clusters Safety
- Add `maxClusters: 40` to config
- In HAC: if `active.size > 40`, force merge closest pair even if over threshold
- Prevents pathological explosion on highly divergent data

## Integration Points

### Your Existing Codebase
- **Offscreen:** Already set up in `OffscreenBootstrap.js` with `BusController.js`
- **Build:** Uses `esbuild` + `scripts/postbuild.js` for static file copying
- **Message bus:** JSON-based (can't send Float32Array directly)

### Solution
- Offscreen returns `number[][]` over the bus
- Service worker rehydrates: `Float32Array.from(arr)`
- Add model copying to `postbuild.js`: `fs.cpSync('models', 'dist/models')`

### Runtime Choice
Use `@xenova/transformers` (not raw ONNX):
- Handles WebGPU/WASM fallback automatically
- Includes tokenizer
- Loads models via `chrome.runtime.getURL('models/...')`

## File Structure You're Creating

```
src/clustering/
├── types.ts              # ClusterableItem, ParagraphCluster, ClusteringResult
├── config.ts             # DEFAULT_CONFIG with maxClusters: 40
├── distance.ts           # cosineSimilarity(), quantizeSimilarity()
├── hac.ts                # hierarchicalCluster() with tie-breakers
├── engine.ts             # buildClusters(), needs shadowStatements param
├── embeddings.ts         # generateEmbeddings(), rehydrates Float32Array
└── index.ts              # clusterParagraphs(paragraphs, shadowStatements)

src/shadow/
└── ShadowParagraphProjector.ts  # projectParagraphs()

src/offscreen/
└── embedding-worker.ts   # WebGPU/WASM inference, returns number[][]
```

## API Signatures (Locked)

```typescript
// Main entry point
export async function clusterParagraphs(
    paragraphs: ShadowParagraph[],
    shadowStatements: ShadowStatement[],  // CRITICAL: needed for unclipped text
    config?: Partial<ClusteringConfig>
): Promise<ClusteringResult>

// Embedding generation
export async function generateEmbeddings(
    paragraphs: ShadowParagraph[],
    shadowStatements: ShadowStatement[],  // CRITICAL: build text from these
    config: ClusteringConfig
): Promise<EmbeddingResult>

// HAC algorithm
export function hierarchicalCluster(
    paragraphIds: string[],
    distances: number[][],
    config: ClusteringConfig
): number[][]  // Array of clusters (each cluster = array of paragraph indices)
```

## Uncertainty Detection (Complete List)

Set `uncertain: true` if **any** of these (check in this order for determinism):
1. `cohesion < 0.70` → `'low_cohesion'`
2. `size > 8` → `'oversized'`
3. `unique stances >= 3` → `'stance_diversity'`
4. `contested_count / size > 0.30` → `'high_contested_ratio'`
5. `has tension signal && has conditional signal` → `'conflicting_signals'`

## StepExecutor Integration

Replace:
```javascript
const shadowResult = extractShadowStatements(shadowInput);
const mappingPrompt = buildSemanticMapperPrompt(userQuery, shadowResult.statements);
```

With:
```javascript
const shadowResult = extractShadowStatements(shadowInput);

// Project paragraphs
const { projectParagraphs } = await import('../../shadow/ShadowParagraphProjector');
const paragraphResult = projectParagraphs(shadowResult.statements);

// Cluster paragraphs (may fail)
let clusteringResult = null;
if (paragraphResult.paragraphs.length >= 3) {
    try {
        const { clusterParagraphs } = await import('../../clustering');
        clusteringResult = await clusterParagraphs(
            paragraphResult.paragraphs,
            shadowResult.statements  // CRITICAL: pass original statements
        );
    } catch (err) {
        console.warn('[StepExecutor] Clustering failed, continuing without:', err);
    }
}

// Build prompt with pre-computed data
const mappingPrompt = buildSemanticMapperPrompt(
    userQuery,
    shadowResult.statements,
    paragraphResult,
    clusteringResult
);
```

## Testing Checklist

1. **Determinism:** Run same input 3x, compare cluster IDs/membership
2. **Provenance:** Verify no `p_*` / `pc_*` in parsed claims
3. **No duplication:** Grep prompt for repeated statement text
4. **Centroid not first:** Verify representative isn't always first paragraph
5. **Cross-model clustering:** UI design query should produce clusters spanning models
6. **Embedding failure:** Mock failure, verify mapping continues

## Common Pitfalls to Avoid

- ❌ Using `paragraph._fullParagraph` for embeddings
- ❌ Using `paragraph.statements[].text` (it's clipped to 320 chars)
- ❌ Forgetting to renormalize after truncation
- ❌ Not quantizing similarities in comparisons
- ❌ Including `representativeText` in cluster object
- ❌ Allowing `p_*` / `pc_*` IDs to pass parser validation

## Start Here

1. **Day 1:** Add `validateProvenance()` to `shared/parsing-utils.ts` and wire into parser
2. **Day 2:** Create `ShadowParagraphProjector.ts` with contested detection
3. **Day 3:** Add `@xenova/transformers` dependency and create `embedding-worker.ts`
4. **Day 4-6:** Implement clustering module with all corrections
5. **Day 7:** Integrate into StepExecutor

You have a complete spec. Follow it exactly, implement the corrections, and you'll have production-grade semantic clustering.