# Complete Clustering Implementation

## Key Design: Identity-Only Clustering (No Judgment, No Optimization)

In one sentence: “At this stage, don’t think, don’t judge, don’t reason, don’t optimize — just answer one pure question: ‘Which pieces of text are really saying the same thing?’”

### Contract (Non-Negotiable)

The clustering layer performs **semantic identity compression only**.

It is forbidden from:
- expressing conflict
- expressing dependency
- expressing conditionality
- enforcing capacity limits
- introducing structural priors (stance-based merge rules, prerequisite/dependent incompatibility, etc.)

It may only answer:
- “Which paragraphs are paraphrases or near-paraphrases of the same claim?”

Paragraphs are treated as a **transport unit**, not a semantic verdict:
- Paragraphs carry multiple atomic shadow statements + provenance.
- Clusters group carriers that talk about the same underlying proposition.
- Statement-level IDs remain preserved for downstream citation and auditing.

Resource pressure (UI/token limits) is handled downstream via pagination / lazy-loading / prioritization, not by forced merges.

```
NOT this (forced):     k-means with k=20
                       ↓
                       Always 20 clusters regardless of data

THIS (emergent):       HAC with similarity threshold
                       ↓
                       3 clusters if high consensus
                       35 clusters if high divergence
                       Whatever the data demands (no hard cap; paginate later)
```

---

## File Structure

```
src/clustering/
├── types.ts              # Type definitions
├── config.ts             # Configuration (threshold + embedding)
├── distance.ts           # Similarity calculations
├── hac.ts                # Hierarchical clustering algorithm
├── clusteringEngine.ts   # Main orchestration
├── clusteringClient.ts   # Service worker API
└── index.ts              # Exports

src/offscreen/
└── embedding-worker.ts   # WebGPU inference
```

---

## File 1: `src/clustering/types.ts`

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// CLUSTERING TYPES
// ═══════════════════════════════════════════════════════════════════════════

import { Stance, SignalFlags } from '../shadow';

/**
 * Input to clustering: what we receive from Shadow V3
 */
export interface ClusterableItem {
    id: string;
    text: string;
    modelIndex: number;
    dominantStance: Stance;
    stanceHints: Stance[];
    signals: SignalFlags;
}

/**
 * A proto-claim: identity-only cluster of similar paragraphs
 *
 * Minimal output: grouping + provenance + cohesion
 */
export interface ProtoClaim {
    id: string;                      // "pc_0", "pc_1", ...
    
    // Constituent paragraphs
    paragraphIds: string[];
    
    centroidParagraphId: string;
    centroidText: string;
    
    cohesion: number;                // 0-1, internal similarity
}

/**
 * Full clustering result
 */
export interface ClusteringResult {
    protoClaims: ProtoClaim[];
    
    meta: {
        // Input stats
        inputParagraphs: number;
        inputModels: number;
        
        // Output stats
        outputClusters: number;
        singletonCount: number;
        avgClusterSize: number;
        maxClusterSize: number;
        
        // Performance
        embeddingTimeMs: number;
        clusteringTimeMs: number;
        totalTimeMs: number;
    };
}

/**
 * Embedding result from offscreen worker
 */
export interface EmbeddingResult {
    embeddings: Float32Array[];
    dimensions: number;
    timeMs: number;
}
```

---

## File 2: `src/clustering/config.ts`

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// CLUSTERING CONFIGURATION - DYNAMIC LIMITS
// ═══════════════════════════════════════════════════════════════════════════

export interface ClusteringConfig {
    /**
     * Below this input count, don't cluster at all.
     */
    minInputForClustering: number;
    
    // ─────────────────────────────────────────────────────────────────────
    // SIMILARITY THRESHOLD (controls emergent cluster count)
    // ─────────────────────────────────────────────────────────────────────
    
    /**
     * Minimum cosine similarity to consider merging (0-1).
     * Higher = more clusters (stricter matching).
     * Lower = fewer clusters (looser matching).
     *
     * Identity-only clustering must be conservative:
     * prefer false splits (extra clusters) over false merges.
     * 
     * 0.80 = very strict, many small clusters
     * 0.70 = balanced
     * 0.60 = loose, fewer larger clusters
     */
    similarityThreshold: number;
    
    // ─────────────────────────────────────────────────────────────────────
    // EMBEDDING
    // ─────────────────────────────────────────────────────────────────────
    
    /**
     * Embedding dimensions (MRL truncation).
     * Lower = faster but less precise.
     */
    embeddingDimensions: number;
    
    /**
     * Model to use for embeddings.
     */
    modelId: string;
}

/**
 * Default configuration: balanced for typical use
 */
export const DEFAULT_CONFIG: ClusteringConfig = {
    minInputForClustering: 3,
    
    // Similarity (conservative by default to avoid false merges)
    similarityThreshold: 0.85,
    
    // Embedding
    embeddingDimensions: 256,
    modelId: 'all-MiniLM-L6-v2',
};

/**
 * Presets for different scenarios
 */
export const CONFIG_PRESETS = {
    /**
     * High precision: More clusters, stricter matching.
     * Use when: Nuanced differences matter, willing to have many small clusters.
     */
    highPrecision: {
        ...DEFAULT_CONFIG,
        similarityThreshold: 0.88,
    } as ClusteringConfig,
    
    /**
     * Balanced: Default behavior.
     * Use when: General purpose, good tradeoff.
     */
    balanced: DEFAULT_CONFIG,
    
    /**
     * High recall: Fewer clusters, looser matching.
     * Use when: Want to group aggressively, reduce token count maximally.
     */
    highRecall: {
        ...DEFAULT_CONFIG,
        similarityThreshold: 0.80,
    } as ClusteringConfig,
    
    /**
     * Fast: Trade quality for speed.
     * Use when: Many paragraphs, latency critical.
     */
    fast: {
        ...DEFAULT_CONFIG,
        embeddingDimensions: 128,
    } as ClusteringConfig,
};

// Adaptive thresholding intentionally removed initially for identity stability.
// Add later only after the downstream pipeline is hardened and evaluated.
```

---

## File 3: `src/clustering/distance.ts`

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// DISTANCE & SIMILARITY CALCULATIONS
// ═══════════════════════════════════════════════════════════════════════════

import { ClusteringConfig } from './config';

/**
 * Cosine similarity between two normalized vectors.
 */
export function cosineSimilarity(a: Float32Array, b: Float32Array): number {
    let dot = 0;
    const len = Math.min(a.length, b.length);
    for (let i = 0; i < len; i++) {
        dot += a[i] * b[i];
    }
    return dot;
}

/**
 * Build full distance matrix (identity-only).
 *
 * NOTE: Stance/signals are intentionally NOT used as clustering forces.
 * They remain annotations attached to members for downstream interpretation.
 */
export function buildDistanceMatrix(
    embeddings: Float32Array[],
    config: ClusteringConfig
): { distances: number[][]; similarities: number[] } {
    const n = embeddings.length;
    const distances: number[][] = Array(n).fill(null).map(() => Array(n).fill(0));
    const similarities: number[] = [];
    
    for (let i = 0; i < n; i++) {
        for (let j = i + 1; j < n; j++) {
            const sim = cosineSimilarity(embeddings[i], embeddings[j]);
            const distance = 1 - sim;
            distances[i][j] = distance;
            distances[j][i] = distance;
            
            similarities.push(sim);
        }
    }
    
    return { distances, similarities };
}

/**
 * Compute silhouette score for clustering quality.
 * Range: -1 (bad) to 1 (good).
 */
export function computeSilhouetteScore(
    clusters: number[][],
    distances: number[][]
): number {
    if (clusters.length <= 1) return 0;
    
    const clusterAssignment = new Map<number, number>();
    clusters.forEach((cluster, clusterIdx) => {
        cluster.forEach(itemIdx => clusterAssignment.set(itemIdx, clusterIdx));
    });
    
    let totalScore = 0;
    let itemCount = 0;
    
    for (const [itemIdx, clusterIdx] of clusterAssignment) {
        const cluster = clusters[clusterIdx];
        
        // a(i) = average distance to points in same cluster
        let a = 0;
        if (cluster.length > 1) {
            for (const other of cluster) {
                if (other !== itemIdx) {
                    a += distances[itemIdx][other];
                }
            }
            a /= (cluster.length - 1);
        }
        
        // b(i) = minimum average distance to points in any other cluster
        let b = Infinity;
        for (let otherClusterIdx = 0; otherClusterIdx < clusters.length; otherClusterIdx++) {
            if (otherClusterIdx === clusterIdx) continue;
            
            const otherCluster = clusters[otherClusterIdx];
            let avgDist = 0;
            for (const other of otherCluster) {
                avgDist += distances[itemIdx][other];
            }
            avgDist /= otherCluster.length;
            
            b = Math.min(b, avgDist);
        }
        
        // Silhouette for this point
        const s = (b - a) / Math.max(a, b);
        totalScore += isNaN(s) ? 0 : s;
        itemCount++;
    }
    
    return itemCount > 0 ? totalScore / itemCount : 0;
}
```

---

## File 4: `src/clustering/hac.ts`

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// HIERARCHICAL AGGLOMERATIVE CLUSTERING
// ═══════════════════════════════════════════════════════════════════════════

import { ClusteringConfig } from './config';

/**
 * Linkage strategy for computing cluster distance.
 */
export type LinkageType = 'average' | 'complete' | 'single';

/**
 * Average linkage: mean distance between all pairs.
 * Generally best for semantic clustering.
 */
function averageLinkage(
    clusterA: Set<number>,
    clusterB: Set<number>,
    distances: number[][]
): number {
    let sum = 0;
    let count = 0;
    
    for (const i of clusterA) {
        for (const j of clusterB) {
            sum += distances[i][j];
            count++;
        }
    }
    
    return count > 0 ? sum / count : Infinity;
}

/**
 * Complete linkage: maximum distance between any pair.
 * Produces tight, compact clusters.
 */
function completeLinkage(
    clusterA: Set<number>,
    clusterB: Set<number>,
    distances: number[][]
): number {
    let maxDist = 0;
    
    for (const i of clusterA) {
        for (const j of clusterB) {
            maxDist = Math.max(maxDist, distances[i][j]);
        }
    }
    
    return maxDist;
}

/**
 * Single linkage: minimum distance between any pair.
 * Can produce elongated clusters (chaining effect).
 */
function singleLinkage(
    clusterA: Set<number>,
    clusterB: Set<number>,
    distances: number[][]
): number {
    let minDist = Infinity;
    
    for (const i of clusterA) {
        for (const j of clusterB) {
            minDist = Math.min(minDist, distances[i][j]);
        }
    }
    
    return minDist;
}

/**
 * Get linkage function by type.
 */
function getLinkage(type: LinkageType) {
    switch (type) {
        case 'average': return averageLinkage;
        case 'complete': return completeLinkage;
        case 'single': return singleLinkage;
    }
}

/**
 * Hierarchical Agglomerative Clustering with threshold-based stopping.
 * 
 * Key: Cluster count is EMERGENT from the data, not forced.
 * The algorithm stops when nothing is similar enough to merge.
 */
export function hierarchicalCluster(
    distances: number[][],
    threshold: number,
    config: ClusteringConfig,
    linkage: LinkageType = 'average'
): number[][] {
    const n = distances.length;
    
    // Edge case: not enough items
    if (n < config.minInputForClustering) {
        return Array.from({ length: n }, (_, i) => [i]);
    }
    
    // Initialize: each item is its own cluster
    let clusters: Set<number>[] = Array.from({ length: n }, (_, i) => new Set([i]));
    let active = new Set(Array.from({ length: n }, (_, i) => i));
    
    const linkageFn = getLinkage(linkage);
    const distanceThreshold = 1 - threshold;  // Convert similarity to distance
    
    // Merge loop
    while (active.size > 1) {
        // Find closest pair
        let minDist = Infinity;
        let minI = -1;
        let minJ = -1;
        
        const activeArray = Array.from(active);
        for (let ai = 0; ai < activeArray.length; ai++) {
            for (let aj = ai + 1; aj < activeArray.length; aj++) {
                const i = activeArray[ai];
                const j = activeArray[aj];
                const dist = linkageFn(clusters[i], clusters[j], distances);
                if (dist < minDist) {
                    minDist = dist;
                    minI = i;
                    minJ = j;
                }
            }
        }
        
        // Stop if closest pair exceeds threshold
        if (minDist > distanceThreshold) {
            break;
        }
        
        // Merge j into i
        for (const idx of clusters[minJ]) {
            clusters[minI].add(idx);
        }
        active.delete(minJ);
    }
    
    // Convert to array format
    return Array.from(active).map(i => Array.from(clusters[i]));
}

/**
 * Compute cluster cohesion (internal similarity).
 */
export function computeClusterCohesion(
    cluster: number[],
    distances: number[][]
): number {
    if (cluster.length <= 1) return 1.0;
    
    let totalSim = 0;
    let pairs = 0;
    
    for (let i = 0; i < cluster.length; i++) {
        for (let j = i + 1; j < cluster.length; j++) {
            totalSim += 1 - distances[cluster[i]][cluster[j]];
            pairs++;
        }
    }
    
    return pairs > 0 ? totalSim / pairs : 1.0;
}
```

---

## File 5: `src/clustering/clusteringEngine.ts`

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// CLUSTERING ENGINE - MAIN ORCHESTRATION
// ═══════════════════════════════════════════════════════════════════════════

import { Stance, SignalFlags, ShadowParagraph } from '../shadow';
import { ClusterableItem, ProtoClaim, ClusteringResult, EmbeddingResult } from './types';
import { ClusteringConfig, DEFAULT_CONFIG } from './config';
import { buildDistanceMatrix } from './distance';
import { hierarchicalCluster, computeClusterCohesion } from './hac';

/**
 * Convert Shadow paragraphs to clusterable items.
 */
export function toClusterableItems(paragraphs: ShadowParagraph[]): ClusterableItem[] {
    return paragraphs
        .filter(p => !p.excluded)
        .map(p => ({
            id: p.id,
            text: p.text,
            modelIndex: p.modelIndex,
            dominantStance: p.dominantStance,
            stanceHints: p.stanceHints,
            signals: p.signals,
        }));
}

/**
 * Find the centroid of a cluster (item closest to cluster mean).
 */
function findCentroid(
    clusterIndices: number[],
    embeddings: Float32Array[]
): { index: number; similarity: number } {
    if (clusterIndices.length === 1) {
        return { index: clusterIndices[0], similarity: 1.0 };
    }
    
    // Compute cluster mean
    const dim = embeddings[0].length;
    const mean = new Float32Array(dim);
    
    for (const idx of clusterIndices) {
        const emb = embeddings[idx];
        for (let d = 0; d < dim; d++) {
            mean[d] += emb[d];
        }
    }
    
    // Normalize mean
    let norm = 0;
    for (let d = 0; d < dim; d++) {
        mean[d] /= clusterIndices.length;
        norm += mean[d] * mean[d];
    }
    norm = Math.sqrt(norm);
    if (norm > 0) {
        for (let d = 0; d < dim; d++) {
            mean[d] /= norm;
        }
    }
    
    // Find item closest to mean
    let bestIdx = clusterIndices[0];
    let bestSim = -Infinity;
    
    for (const idx of clusterIndices) {
        let sim = 0;
        const emb = embeddings[idx];
        for (let d = 0; d < dim; d++) {
            sim += emb[d] * mean[d];
        }
        if (sim > bestSim) {
            bestSim = sim;
            bestIdx = idx;
        }
    }
    
    return { index: bestIdx, similarity: bestSim };
}

/**
 * Aggregate signals from cluster members (OR logic).
 */
/**
 * Main clustering function.
 * 
 * Takes items and embeddings, returns proto-claims.
 * Cluster count is determined by the data, not forced.
 */
export function buildProtoClaims(
    items: ClusterableItem[],
    embeddings: Float32Array[],
    config: ClusteringConfig = DEFAULT_CONFIG
): { protoClaims: ProtoClaim[]; meta: ClusteringResult['meta'] } {
    const startTime = performance.now();
    
    // Edge case: too few items
    if (items.length < config.minInputForClustering) {
        return {
            protoClaims: items.map((item, idx) => ({
                id: `pc_${idx}`,
                paragraphIds: [item.id],
                centroidParagraphId: item.id,
                centroidText: item.text,
                cohesion: 1.0,
            })),
            meta: buildMeta(items, items.map((_, i) => [i]), 0, performance.now() - startTime),
        };
    }
    
    // Build distance matrix
    const { distances } = buildDistanceMatrix(embeddings, config);
    const threshold = config.similarityThreshold;
    
    // Perform clustering
    const clusterIndices = hierarchicalCluster(distances, threshold, config);
    
    // Build proto-claims from clusters
    const protoClaims: ProtoClaim[] = clusterIndices.map((indices, clusterIdx) => {
        const clusterItems = indices.map(i => items[i]);
        
        // Find centroid
        const centroid = findCentroid(indices, embeddings);
        const centroidItem = items[centroid.index];
        
        // Compute cohesion
        const cohesion = computeClusterCohesion(indices, distances);
        
        return {
            id: `pc_${clusterIdx}`,
            paragraphIds: clusterItems.map(i => i.id),
            centroidParagraphId: centroidItem.id,
            centroidText: centroidItem.text,
            cohesion,
        };
    });
    
    // Sort by size (largest first) for consistent ordering
    protoClaims.sort((a, b) => b.paragraphIds.length - a.paragraphIds.length);

    
    // Renumber IDs after sorting
    protoClaims.forEach((pc, idx) => {
        pc.id = `pc_${idx}`;
    });
    
    const clusteringTime = performance.now() - startTime;
    
    return {
        protoClaims,
        meta: buildMeta(items, clusterIndices, clusteringTime, 0),
    };
}

/**
 * Build metadata about clustering result.
 */
function buildMeta(
    items: ClusterableItem[],
    clusters: number[][],
    clusteringTimeMs: number,
    embeddingTimeMs: number
): ClusteringResult['meta'] {
    const singletons = clusters.filter(c => c.length === 1);
    const totalModels = new Set(items.map(i => i.modelIndex)).size;
    
    return {
        inputParagraphs: items.length,
        inputModels: totalModels,
        outputClusters: clusters.length,
        singletonCount: singletons.length,
        avgClusterSize: items.length / clusters.length,
        maxClusterSize: Math.max(...clusters.map(c => c.length)),
        embeddingTimeMs,
        clusteringTimeMs,
        totalTimeMs: embeddingTimeMs + clusteringTimeMs,
    };
}
```

---

## File 6: `src/clustering/clusteringClient.ts`

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// CLUSTERING CLIENT - SERVICE WORKER API
// ═══════════════════════════════════════════════════════════════════════════

import { ShadowParagraph } from '../shadow';
import { ClusteringResult } from './types';
import { ClusteringConfig, DEFAULT_CONFIG, CONFIG_PRESETS } from './config';
import { toClusterableItems, buildProtoClaims } from './clusteringEngine';

/**
 * Ensure offscreen document is ready for embedding inference.
 */
async function ensureOffscreen(): Promise<void> {
    const existingContexts = await chrome.runtime.getContexts({
        contextTypes: ['OFFSCREEN_DOCUMENT' as any],
    });
    
    if (existingContexts.length > 0) return;
    
    await chrome.offscreen.createDocument({
        url: 'offscreen.html',
        reasons: ['WORKERS' as any],
        justification: 'Embedding model inference for semantic clustering',
    });
}

/**
 * Request embeddings from offscreen worker.
 */
async function requestEmbeddings(
    texts: string[],
    dimensions: number
): Promise<{ embeddings: Float32Array[]; timeMs: number }> {
    await ensureOffscreen();
    
    return new Promise((resolve, reject) => {
        chrome.runtime.sendMessage(
            {
                type: 'GENERATE_EMBEDDINGS',
                payload: { texts, dimensions },
            },
            (response) => {
                if (chrome.runtime.lastError) {
                    reject(new Error(chrome.runtime.lastError.message));
                } else if (response?.success) {
                    // Convert back to Float32Array (serialization loses type)
                    const embeddings = response.result.embeddings.map(
                        (arr: number[]) => new Float32Array(arr)
                    );
                    resolve({
                        embeddings,
                        timeMs: response.result.timeMs,
                    });
                } else {
                    reject(new Error(response?.error || 'Embedding generation failed'));
                }
            }
        );
    });
}

/**
 * Preload the embedding model (call during idle time).
 */
export async function preloadClusteringModel(): Promise<void> {
    await ensureOffscreen();
    
    return new Promise((resolve, reject) => {
        chrome.runtime.sendMessage(
            { type: 'PRELOAD_MODEL' },
            (response) => {
                if (chrome.runtime.lastError) {
                    reject(new Error(chrome.runtime.lastError.message));
                } else if (response?.success) {
                    resolve();
                } else {
                    reject(new Error(response?.error || 'Model preload failed'));
                }
            }
        );
    });
}

/**
 * Main entry point: Cluster shadow paragraphs into proto-claims.
 * 
 * Cluster count is DYNAMIC based on semantic identity only.
 * Will produce anywhere from 1 to N clusters (no hard cap).
 *
 * If UI/token limits are exceeded, paginate / lazy-load downstream.
 */
export async function clusterParagraphs(
    paragraphs: ShadowParagraph[],
    config: Partial<ClusteringConfig> = {}
): Promise<ClusteringResult> {
    const mergedConfig: ClusteringConfig = { ...DEFAULT_CONFIG, ...config };
    const startTime = performance.now();
    
    // Convert to clusterable items
    const items = toClusterableItems(paragraphs);
    
    // Edge case: too few items
    if (items.length < mergedConfig.minInputForClustering) {
        const trivialResult = buildProtoClaims(items, [], mergedConfig);
        return {
            ...trivialResult,
            meta: {
                ...trivialResult.meta,
                totalTimeMs: performance.now() - startTime,
            },
        };
    }
    
    // Get embeddings from offscreen worker
    const texts = items.map(i => i.text);
    const { embeddings, timeMs: embeddingTimeMs } = await requestEmbeddings(
        texts,
        mergedConfig.embeddingDimensions
    );
    
    // Perform clustering
    const { protoClaims, meta } = buildProtoClaims(items, embeddings, mergedConfig);
    
    return {
        protoClaims,
        meta: {
            ...meta,
            embeddingTimeMs,
            totalTimeMs: performance.now() - startTime,
        },
    };
}

/**
 * Convenience: Cluster with a preset configuration.
 */
export async function clusterWithPreset(
    paragraphs: ShadowParagraph[],
    preset: keyof typeof CONFIG_PRESETS
): Promise<ClusteringResult> {
    return clusterParagraphs(paragraphs, CONFIG_PRESETS[preset]);
}
```

---

## File 7: `src/offscreen/embedding-worker.ts`

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// EMBEDDING WORKER - OFFSCREEN DOCUMENT
// ═══════════════════════════════════════════════════════════════════════════

import { env, pipeline, Pipeline } from '@huggingface/transformers';

// ═══════════════════════════════════════════════════════════════════════════
// CONFIGURATION
// ═══════════════════════════════════════════════════════════════════════════

// Use local models (true offline support)
env.allowRemoteModels = false;
env.localModelPath = chrome.runtime.getURL('models/');

// Prefer WebGPU, fall back to WASM
env.backends.onnx.wasm.numThreads = 4;

let embedder: Pipeline | null = null;
let isInitializing = false;
let initPromise: Promise<void> | null = null;

// ═══════════════════════════════════════════════════════════════════════════
// MODEL INITIALIZATION
// ═══════════════════════════════════════════════════════════════════════════

async function initializeModel(): Promise<void> {
    if (embedder) return;
    if (initPromise) return initPromise;
    
    isInitializing = true;
    initPromise = (async () => {
        console.log('[Embedder] Loading model...');
        const startTime = performance.now();
        
        try {
            embedder = await pipeline(
                'feature-extraction',
                'all-MiniLM-L6-v2',
                {
                    device: 'webgpu',
                    dtype: 'q8',
                }
            );
            console.log(`[Embedder] Loaded with WebGPU in ${Math.round(performance.now() - startTime)}ms`);
        } catch (webgpuError) {
            console.warn('[Embedder] WebGPU unavailable, falling back to WASM:', webgpuError);
            
            embedder = await pipeline(
                'feature-extraction',
                'all-MiniLM-L6-v2',
                {
                    device: 'wasm',
                    dtype: 'q8',
                }
            );
            console.log(`[Embedder] Loaded with WASM in ${Math.round(performance.now() - startTime)}ms`);
        }
        
        isInitializing = false;
    })();
    
    return initPromise;
}

// ═══════════════════════════════════════════════════════════════════════════
// EMBEDDING GENERATION
// ═══════════════════════════════════════════════════════════════════════════

async function generateEmbeddings(
    texts: string[],
    targetDimensions: number
): Promise<{ embeddings: number[][]; timeMs: number }> {
    await initializeModel();
    
    const startTime = performance.now();
    
    if (!embedder) {
        throw new Error('Embedder not initialized');
    }
    
    // Process in batches to avoid memory issues
    const batchSize = 32;
    const allEmbeddings: number[][] = [];
    
    for (let i = 0; i < texts.length; i += batchSize) {
        const batch = texts.slice(i, i + batchSize);
        
        const outputs = await embedder(batch, {
            pooling: 'mean',
            normalize: true,
        });
        
        // Extract and truncate embeddings
        for (let j = 0; j < batch.length; j++) {
            const output = outputs[j];
            const data = output.data as Float32Array;
            
            // Truncate to target dimensions (MRL)
            const truncated = Array.from(data.slice(0, targetDimensions));
            
            // Re-normalize after truncation
            let norm = 0;
            for (const v of truncated) norm += v * v;
            norm = Math.sqrt(norm);
            if (norm > 0) {
                for (let k = 0; k < truncated.length; k++) {
                    truncated[k] /= norm;
                }
            }
            
            allEmbeddings.push(truncated);
        }
    }
    
    return {
        embeddings: allEmbeddings,
        timeMs: performance.now() - startTime,
    };
}

// ═══════════════════════════════════════════════════════════════════════════
// MESSAGE HANDLER
// ═══════════════════════════════════════════════════════════════════════════

chrome.runtime.onMessage.addListener((message, _sender, sendResponse) => {
    if (message.type === 'GENERATE_EMBEDDINGS') {
        const { texts, dimensions } = message.payload;
        
        generateEmbeddings(texts, dimensions)
            .then(result => sendResponse({ success: true, result }))
            .catch(error => sendResponse({ success: false, error: error.message }));
        
        return true;  // Async response
    }
    
    if (message.type === 'PRELOAD_MODEL') {
        initializeModel()
            .then(() => sendResponse({ success: true }))
            .catch(error => sendResponse({ success: false, error: error.message }));
        
        return true;  // Async response
    }
    
    return false;
});

// Preload on document ready
initializeModel().catch(err => {
    console.warn('[Embedder] Background preload failed:', err);
});
```

---

## File 8: `src/clustering/index.ts`

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// CLUSTERING MODULE - PUBLIC API
// ═══════════════════════════════════════════════════════════════════════════

// Types
export type {
    ClusterableItem,
    ProtoClaim,
    ClusteringResult,
    EmbeddingResult,
} from './types';

// Configuration
export {
    ClusteringConfig,
    DEFAULT_CONFIG,
    CONFIG_PRESETS,
} from './config';

// Main API
export {
    clusterParagraphs,
    clusterWithPreset,
    preloadClusteringModel,
} from './clusteringClient';

// Engine (for testing/advanced use)
export {
    toClusterableItems,
    buildProtoClaims,
} from './clusteringEngine';
```

---

## Usage Examples

### Basic Usage

```typescript
import { clusterParagraphs } from './clustering';
import { extractShadowParagraphs } from './shadow';

const shadowResult = extractShadowParagraphs(batchResponses);

// Cluster count is AUTOMATIC based on data
const clusteringResult = await clusterParagraphs(shadowResult.paragraphs);

console.log(`Created ${clusteringResult.protoClaims.length} clusters from ${clusteringResult.meta.inputParagraphs} paragraphs`);
// Could be 5 clusters (high consensus) or 35 clusters (high divergence)
```

### With Custom Configuration

```typescript
import { clusterParagraphs, DEFAULT_CONFIG } from './clustering';

const result = await clusterParagraphs(paragraphs, {
    // Stricter matching = more clusters
    similarityThreshold: 0.80,
});
```

### With Presets

```typescript
import { clusterWithPreset } from './clustering';

// High precision: many small clusters
const precise = await clusterWithPreset(paragraphs, 'highPrecision');

// High recall: fewer larger clusters  
const grouped = await clusterWithPreset(paragraphs, 'highRecall');
```

---

## Expected Cluster Counts (Examples)

| Scenario | Paragraphs | Expected Clusters | Why |
|----------|------------|-------------------|-----|
| High consensus | 50 | 8-12 | Most items cluster together |
| Balanced | 50 | 18-25 | Typical divergence |
| High divergence | 50 | 30+ | Many unique positions |
| Very short responses | 15 | 5-10 | Not much to cluster |
| Very long responses | 80 | 25+ | More positions expressed |

The algorithm **discovers** the right number based on:
1. Semantic similarity distribution
2. Conservative fixed threshold (identity stability)

If N clusters are produced, N clusters are returned. No forced merges.
Downstream layers handle capacity via pagination / lazy rendering / prioritization.
