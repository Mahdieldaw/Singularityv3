# Consolidated Implementation Plan: Embedding-Based Paragraph Clustering

**Version:** 1.1 Final (with Critical Corrections)  
**Status:** Ready for Implementation  
**Estimated Scope:** 13 new files, 7 modified files, 10+ files to inspect

---

## Executive Summary

### What We're Building
A semantic clustering layer between Shadow extraction and Semantic Mapping that groups paragraphs by meaning (not vocabulary) using embedding similarity.

### Why
Current Jaccard-based clustering fails when different LLMs express the same idea with different words. In the UI design query test case, two models described identical 3-zone layouts but Jaccard found zero overlap, producing 100% singletons.

### Key Decisions (Locked)
| Decision | Value | Rationale |
|----------|-------|-----------|
| Clustering method | Embedding cosine similarity | Jaccard fails on paraphrases |
| Clustering unit | Paragraphs | Sentences too fragmented |
| Similarity threshold | 0.82 | Moderately conservative |
| Representative selection | Centroid | Most semantically central |
| Async boundary | StepExecutor pre-computes | Prompt builder stays sync |
| Failure behavior | Skip clustering entirely | No Jaccard fallback |
| Backend priority | WebGPU > WASM | Performance with fallback |
| Model bundling | At build time | Offline support |
| Max clusters | 40 | Safety limit (v1.1) |
| Determinism | Quantized similarity | Prevents drift (v1.1) |

---

## Version 1.1 Changes

### Critical Corrections Applied

1. **Phase 0 Added: Parser Enforcement**
   - Hard errors on `p_*` / `pc_*` citations
   - Validates all `sourceStatementIds` against known statements

2. **Quantized Similarity (Determinism)**
   - All similarity comparisons use `Math.round(sim * 1e6) / 1e6`
   - Prevents floating-point drift across runs

3. **Stable Tie-Breakers in HAC**
   - When similarities equal, choose lexicographically smallest IDs
   - Ensures same input → same output

4. **Max Clusters Safety Limit**
   - Added `maxClusters: 40` to configuration
   - Prevents pathological explosion on highly divergent data

5. **Embedding Input Text Fix**
   - Build from original `ShadowStatement.text` (unclipped)
   - NOT from `_fullParagraph` or prompt-clipped text
   - Clustering API now requires `shadowStatements` parameter

---

## File Manifest

### New Files to Create

| File Path | Purpose | Lines |
|-----------|---------|-------|
| `src/shadow/ShadowParagraphProjector.ts` | Paragraph projection with contested detection | ~250 |
| `src/clustering/types.ts` | Type definitions | ~80 |
| `src/clustering/config.ts` | Thresholds and presets | ~60 |
| `src/clustering/distance.ts` | Cosine similarity, quantization | ~120 |
| `src/clustering/hac.ts` | Hierarchical clustering algorithm | ~150 |
| `src/clustering/engine.ts` | Main orchestration, centroid, uncertainty | ~400 |
| `src/clustering/embeddings.ts` | Offscreen communication | ~150 |
| `src/clustering/index.ts` | Public exports | ~50 |
| `src/offscreen/embedding-worker.ts` | WebGPU/WASM inference | ~200 |
| `models/all-MiniLM-L6-v2/` | ONNX model files directory | - |

### Files to Modify

| File Path | Changes | Impact |
|-----------|---------|--------|
| `src/shadow/ShadowExtractor.ts` | Add markdown/table filtering | Medium |
| `src/shadow/index.ts` | Export projector | Small |
| `src/ConciergeService/semanticMapper.ts` | Accept pre-computed data, remove duplication | Large |
| `StepExecutor.js` | Add projection + clustering before prompt build | Medium |
| `shared/parsing-utils.ts` | Add provenance validation (Phase 0) | Medium |
| `manifest.json` | Verify offscreen permission, web_accessible_resources | Small |
| `package.json` | Add @xenova/transformers | Small |

### Files to Inspect

| File Path | Verify |
|-----------|--------|
| `offscreen.html` | Loads embedding worker |
| `scripts/postbuild.js` | Model file copying |
| `src/ConciergeService/contract.ts` | SemanticMapperOutput interface |
| `src/ConciergeService/claimAssembly.ts` | Handles new claim structure |
| `BusController.js` | JSON serialization limitation |
| `OffscreenBootstrap.js` | Message handler registration |

---

## Phase 0: Parser Enforcement (NEW - Critical First Step)

### Problem
Current parser only **warns** on unknown statement IDs. This allows hallucinated citations (`p_*`, `pc_*`, or invented `s_*` IDs) to pass validation and be accepted as valid output.

### File: `shared/parsing-utils.ts` (MODIFY)

**Add new validation function:**

```typescript
/**
 * Validate that all citations reference valid statement IDs.
 * 
 * CRITICAL: Paragraph (p_*) and cluster (pc_*) IDs are NOT valid citations.
 * Only statement IDs (s_*) can be cited in sourceStatementIds.
 */
export interface ProvenanceError {
    field: string;
    claimId?: string;
    issue: string;
    severity: 'error' | 'warning';
}

export function validateProvenance(
    claims: SemanticClaim[],
    validStatementIds: Set<string>
): ProvenanceError[] {
    const errors: ProvenanceError[] = [];
    
    for (const claim of claims) {
        // Validate claim sourceStatementIds
        for (const id of claim.sourceStatementIds || []) {
            // HARD ERROR: paragraph/cluster IDs not allowed
            if (id.startsWith('p_') || id.startsWith('pc_')) {
                errors.push({
                    field: 'sourceStatementIds',
                    claimId: claim.id,
                    issue: `Invalid ID: ${id}. Only statement IDs (s_*) are citable. Paragraph (p_*) and cluster (pc_*) IDs are internal hints only.`,
                    severity: 'error',
                });
            }
            // STRICT: unknown statement IDs
            else if (!validStatementIds.has(id)) {
                errors.push({
                    field: 'sourceStatementIds',
                    claimId: claim.id,
                    issue: `Unknown statement ID: ${id}. This ID was not found in the shadow extraction output.`,
                    severity: 'error',
                });
            }
        }
        
        // Validate gate sourceStatementIds
        const allGates = [
            ...(claim.gates?.conditionals || []),
            ...(claim.gates?.prerequisites || [])
        ];
        
        for (const gate of allGates) {
            for (const id of gate.sourceStatementIds || []) {
                if (id.startsWith('p_') || id.startsWith('pc_')) {
                    errors.push({
                        field: 'gates.sourceStatementIds',
                        claimId: claim.id,
                        issue: `Gate cites invalid ID: ${id}. Only s_* IDs allowed.`,
                        severity: 'error',
                    });
                } else if (!validStatementIds.has(id)) {
                    errors.push({
                        field: 'gates.sourceStatementIds',
                        claimId: claim.id,
                        issue: `Gate cites unknown statement ID: ${id}`,
                        severity: 'error',
                    });
                }
            }
        }
        
        // Validate conflict sourceStatementIds
        for (const conflict of claim.conflicts || []) {
            for (const id of conflict.sourceStatementIds || []) {
                if (id.startsWith('p_') || id.startsWith('pc_')) {
                    errors.push({
                        field: 'conflicts.sourceStatementIds',
                        claimId: claim.id,
                        issue: `Conflict cites invalid ID: ${id}. Only s_* IDs allowed.`,
                        severity: 'error',
                    });
                } else if (!validStatementIds.has(id)) {
                    errors.push({
                        field: 'conflicts.sourceStatementIds',
                        claimId: claim.id,
                        issue: `Conflict cites unknown statement ID: ${id}`,
                        severity: 'error',
                    });
                }
            }
        }
    }
    
    return errors;
}
```

**Modify `parseSemanticMapperOutput()` to use it:**

```typescript
export function parseSemanticMapperOutput(
    rawOutput: string,
    validStatementIds?: string[]
): ParseResult<SemanticMapperOutput> {
    // ... existing parsing logic ...
    
    // NEW: Validate provenance if statement IDs provided
    if (validStatementIds && parsed.claims) {
        const validIds = new Set(validStatementIds);
        const provenanceErrors = validateProvenance(parsed.claims, validIds);
        
        // Convert provenance errors to parse errors
        for (const err of provenanceErrors) {
            if (err.severity === 'error') {
                errors.push({
                    field: err.field,
                    issue: err.issue,
                    claimId: err.claimId,
                });
            }
        }
    }
    
    // ... rest of validation ...
}
```

### Integration Point

**In `semanticMapper.ts` wrapper:**

```typescript
const validStatementIds = shadowStatements.map(s => s.id);
const parseResult = parseSemanticMapperOutput(
    llmResponse,
    validStatementIds  // NEW: Pass for validation
);

if (!parseResult.success) {
    throw new Error(`Semantic mapping failed validation: ${JSON.stringify(parseResult.errors)}`);
}
```

---

## Phase 1: Shadow Extractor Fixes

### Problem
Markdown headers and table fragments extracted as statements, polluting paragraph projection and clustering.

### File: `src/shadow/ShadowExtractor.ts` (MODIFY)

**Modify `isSubstantive()` function:**

```typescript
function isSubstantive(sentence: string): boolean {
    const trimmed = sentence.trim();
    const words = trimmed.split(/\s+/).filter(w => w.length > 0);

    // Length checks
    if (words.length < 5) return false;

    // ────────────────────────────────────────────────────────────────────────
    // NEW: Filter markdown headers (any level)
    // ────────────────────────────────────────────────────────────────────────
    if (/^#{1,6}\s/.test(trimmed)) return false;
    
    // NEW: Filter bold-only lines (likely titles)
    if (/^\*{2}[^*]+\*{2}$/.test(trimmed)) return false;
    if (/^__[^_]+__$/.test(trimmed)) return false;
    
    // NEW: Filter table fragments
    if (/^\|.*\|$/.test(trimmed) && trimmed.split('|').length > 2) return false;
    if (/^[\|\s\-:]+$/.test(trimmed)) return false;  // Table separator lines
    
    // NEW: Filter list markers alone
    if (/^[-*+]\s*$/.test(trimmed)) return false;
    if (/^\d+\.\s*$/.test(trimmed)) return false;

    // ────────────────────────────────────────────────────────────────────────
    // Existing meta-commentary filters
    // ────────────────────────────────────────────────────────────────────────
    const metaPatterns = [
        /^(sure|okay|yes|no|well|so|now)[,.]?\s/i,
        /^(let me|I'll|I will|I can|I would)\b/i,
        /^(here's|here is|this is|that's|that is)\s+(a|an|the|my)\s+(summary|overview|breakdown|list)/i,
        /\b(as I mentioned|as discussed|as noted)\b/i,
        /^(to summarize|in summary|in conclusion)\b/i,
    ];

    for (const pattern of metaPatterns) {
        if (pattern.test(sentence)) return false;
    }

    return true;
}
```

**Modify `splitIntoSentences()` to handle tables:**

```typescript
function splitIntoSentences(paragraph: string): string[] {
    // ────────────────────────────────────────────────────────────────────────
    // NEW: Detect and skip markdown tables entirely
    // ────────────────────────────────────────────────────────────────────────
    const lines = paragraph.split('\n');
    const hasTableStructure = lines.some(line => 
        /^\|.*\|$/.test(line.trim()) && line.includes('|')
    );
    const hasTableSeparator = lines.some(line => 
        /^[\|\s\-:]+$/.test(line.trim())
    );
    
    if (hasTableStructure && hasTableSeparator) {
        // Skip tables - they fragment poorly
        console.log('[Shadow] Skipping table paragraph');
        return [];
    }

    // ────────────────────────────────────────────────────────────────────────
    // Existing logic
    // ────────────────────────────────────────────────────────────────────────
    const protectedText = paragraph
        .replace(/\b(Mr|Mrs|Ms|Dr|Prof|Inc|Ltd|vs|etc|e\.g|i\.e)\./gi, '$1|||')
        .replace(/\b(\d+)\./g, '$1|||');

    const sentences = protectedText
        .split(/(?<=[.!?])\s+/)
        .map(s => s.replace(/\|\|\|/g, '.'))
        .map(s => s.trim())
        .filter(s => s.length > 0);

    return sentences;
}
```

---

## Phase 2: Paragraph Projector

### File: `src/shadow/ShadowParagraphProjector.ts` (NEW)

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// SHADOW PARAGRAPH PROJECTOR
// ═══════════════════════════════════════════════════════════════════════════
//
// Projects sentence-level Shadow statements onto paragraph-level semantic units.
// Paragraphs are derived (not authoritative) - statements remain source of truth.
//
// Key features:
// - Deterministic paragraph ID assignment (p_0, p_1, ...)
// - Contested detection (contradiction pairs within paragraph)
// - Dominant stance computation (priority-based or confidence-weighted)
// - Signal aggregation (OR logic)
// - Statement ordering preservation (sentence index order)
// ═══════════════════════════════════════════════════════════════════════════

import { ShadowStatement, Stance, STANCE_PRIORITY } from './StatementTypes';

// ═══════════════════════════════════════════════════════════════════════════
// TYPES
// ═══════════════════════════════════════════════════════════════════════════

export interface ShadowParagraph {
    id: string;                      // "p_0", "p_1", ...
    modelIndex: number;
    paragraphIndex: number;
    
    // Provenance
    statementIds: string[];          // s_* IDs in original sentence order
    
    // Aggregated metadata
    dominantStance: Stance;
    stanceHints: Stance[];           // All unique stances present
    contested: boolean;              // True if contradiction pairs exist
    confidence: number;              // max(member.confidence)
    
    signals: {
        sequence: boolean;
        tension: boolean;
        conditional: boolean;
    };
    
    // Content for prompt (statements only, no duplication)
    statements: Array<{
        id: string;
        text: string;                // Clipped ≤320 chars for prompt display
        stance: Stance;
        signals: string[];           // ["SEQ", "TENS", "COND"]
    }>;
    
    // Internal only - NOT sent to prompt, used for expansion
    _fullParagraph: string;
}

export interface ParagraphProjectionResult {
    paragraphs: ShadowParagraph[];
    meta: {
        totalParagraphs: number;
        byModel: Record<number, number>;
        contestedCount: number;
        avgStatementsPerParagraph: number;
        processingTimeMs: number;
    };
}

// ═══════════════════════════════════════════════════════════════════════════
// CONSTANTS
// ═══════════════════════════════════════════════════════════════════════════

const MAX_STATEMENT_TEXT_CHARS = 320;

/**
 * Contradiction pairs that indicate contested paragraph.
 * These represent fundamental disagreements within a single paragraph.
 */
const CONTRADICTION_PAIRS: Array<[Stance, Stance]> = [
    ['prescriptive', 'cautionary'],
    ['assertive', 'uncertain'],
];

// ═══════════════════════════════════════════════════════════════════════════
// HELPERS
// ═══════════════════════════════════════════════════════════════════════════

function clipText(text: string, maxChars: number): string {
    if (text.length <= maxChars) return text;
    return text.slice(0, maxChars).trim() + '...';
}

function isContested(stances: Stance[]): boolean {
    const stanceSet = new Set(stances);
    for (const [a, b] of CONTRADICTION_PAIRS) {
        if (stanceSet.has(a) && stanceSet.has(b)) return true;
    }
    return false;
}

function computeDominantStance(
    statements: Array<{ stance: Stance; confidence: number }>
): Stance {
    if (statements.length === 0) return 'assertive';
    
    const stances = statements.map(s => s.stance);
    
    // If contested, use priority order
    if (isContested(stances)) {
        for (const priorityStance of STANCE_PRIORITY) {
            if (stances.includes(priorityStance)) {
                return priorityStance;
            }
        }
    }
    
    // Otherwise, use weighted confidence
    const stanceScores = new Map<Stance, number>();
    for (const stmt of statements) {
        const current = stanceScores.get(stmt.stance) || 0;
        stanceScores.set(stmt.stance, current + stmt.confidence);
    }
    
    let bestStance: Stance = 'assertive';
    let bestScore = -Infinity;
    
    for (const [stance, score] of stanceScores.entries()) {
        if (score > bestScore) {
            bestScore = score;
            bestStance = stance;
        } else if (score === bestScore) {
            // Tie-break by priority
            const bestIdx = STANCE_PRIORITY.indexOf(bestStance);
            const currIdx = STANCE_PRIORITY.indexOf(stance);
            if (currIdx < bestIdx) {
                bestStance = stance;
            }
        }
    }
    
    return bestStance;
}

function formatSignals(signals: { sequence: boolean; tension: boolean; conditional: boolean }): string[] {
    const result: string[] = [];
    if (signals.sequence) result.push('SEQ');
    if (signals.tension) result.push('TENS');
    if (signals.conditional) result.push('COND');
    return result;
}

// ═══════════════════════════════════════════════════════════════════════════
// MAIN FUNCTION
// ═══════════════════════════════════════════════════════════════════════════

export function projectParagraphs(
    shadowStatements: ShadowStatement[]
): ParagraphProjectionResult {
    const startTime = performance.now();
    
    // ────────────────────────────────────────────────────────────────────────
    // Group by (modelIndex, paragraphIndex)
    // ────────────────────────────────────────────────────────────────────────
    const groups = new Map<string, Array<{
        stmt: ShadowStatement;
        sentenceIndex: number;
        encounter: number;
    }>>();
    
    for (let i = 0; i < shadowStatements.length; i++) {
        const stmt = shadowStatements[i];
        const paragraphIndex = stmt.location?.paragraphIndex ?? 0;
        const sentenceIndex = stmt.location?.sentenceIndex ?? i;
        const key = `${stmt.modelIndex}:${paragraphIndex}`;
        
        if (!groups.has(key)) {
            groups.set(key, []);
        }
        groups.get(key)!.push({ stmt, sentenceIndex, encounter: i });
    }
    
    // ────────────────────────────────────────────────────────────────────────
    // Sort keys for deterministic ordering
    // ────────────────────────────────────────────────────────────────────────
    const sortedKeys = Array.from(groups.keys()).sort((a, b) => {
        const [modelA, paraA] = a.split(':').map(Number);
        const [modelB, paraB] = b.split(':').map(Number);
        return (modelA - modelB) || (paraA - paraB);
    });
    
    const paragraphs: ShadowParagraph[] = [];
    const byModel: Record<number, number> = {};
    let contestedCount = 0;
    
    // ────────────────────────────────────────────────────────────────────────
    // Build paragraph objects
    // ────────────────────────────────────────────────────────────────────────
    for (let i = 0; i < sortedKeys.length; i++) {
        const key = sortedKeys[i];
        const [modelIndexStr, paragraphIndexStr] = key.split(':');
        const modelIndex = Number(modelIndexStr);
        const paragraphIndex = Number(paragraphIndexStr);
        
        // Sort statements within paragraph by sentence order
        const group = groups.get(key)!.sort((a, b) => 
            (a.sentenceIndex - b.sentenceIndex) || (a.encounter - b.encounter)
        );
        
        const statementIds = group.map(g => g.stmt.id);
        const stances = group.map(g => g.stmt.stance);
        const stanceHints = [...new Set(stances)] as Stance[];
        const contested = isContested(stances);
        
        if (contested) contestedCount++;
        
        // Aggregate signals (OR logic)
        const signals = {
            sequence: group.some(g => g.stmt.signals.sequence),
            tension: group.some(g => g.stmt.signals.tension),
            conditional: group.some(g => g.stmt.signals.conditional),
        };
        
        // Compute dominant stance
        const stmtData = group.map(g => ({
            stance: g.stmt.stance,
            confidence: g.stmt.confidence,
        }));
        const dominantStance = computeDominantStance(stmtData);
        
        // Max confidence
        const confidence = Math.max(...group.map(g => g.stmt.confidence));
        
        // Build statement array for prompt (clipped text)
        const statements = group.map(g => ({
            id: g.stmt.id,
            text: clipText(g.stmt.text, MAX_STATEMENT_TEXT_CHARS),
            stance: g.stmt.stance,
            signals: formatSignals(g.stmt.signals),
        }));
        
        // Get full paragraph text (from first statement, unclipped)
        const _fullParagraph = group[0]?.stmt.fullParagraph || '';
        
        paragraphs.push({
            id: `p_${i}`,
            modelIndex,
            paragraphIndex,
            statementIds,
            dominantStance,
            stanceHints,
            contested,
            confidence,
            signals,
            statements,
            _fullParagraph,
        });
        
        // Count by model
        byModel[modelIndex] = (byModel[modelIndex] || 0) + 1;
    }
    
    const totalStatements = shadowStatements.length;
    const avgStatementsPerParagraph = paragraphs.length > 0 
        ? totalStatements / paragraphs.length 
        : 0;
    
    return {
        paragraphs,
        meta: {
            totalParagraphs: paragraphs.length,
            byModel,
            contestedCount,
            avgStatementsPerParagraph,
            processingTimeMs: performance.now() - startTime,
        },
    };
}
```

### File: `src/shadow/index.ts` (MODIFY)

Add exports:

```typescript
// Paragraph projection
export {
    projectParagraphs,
} from './ShadowParagraphProjector';

export type {
    ShadowParagraph,
    ParagraphProjectionResult,
} from './ShadowParagraphProjector';
```

---

## Phase 3: Clustering Module

### File: `src/clustering/types.ts` (NEW)

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// CLUSTERING TYPES
// ═══════════════════════════════════════════════════════════════════════════

import { Stance } from '../shadow';

export interface ClusterableItem {
    id: string;                      // Paragraph ID (p_*)
    text: string;                    // Full paragraph text for embedding
    modelIndex: number;
    dominantStance: Stance;
    stanceHints: Stance[];
    contested: boolean;
    signals: {
        sequence: boolean;
        tension: boolean;
        conditional: boolean;
    };
    statementIds: string[];          // For provenance tracking
}

export interface ParagraphCluster {
    id: string;                      // "pc_0", "pc_1", ...
    paragraphIds: string[];          // Member paragraph IDs in encounter order
    statementIds: string[];          // Union of all statement IDs, stable order
    
    // Representative (centroid)
    representativeParagraphId: string;
    
    // Quality metrics
    cohesion: number;                // 0-1, average similarity to centroid
    
    // Uncertainty detection
    uncertain: boolean;
    uncertaintyReasons: string[];    // ["low_cohesion", "stance_diversity", "oversized", "high_contested_ratio", "conflicting_signals"]
    
    // Expansion (only when uncertain=true)
    expansion?: {
        members: Array<{
            paragraphId: string;
            text: string;            // Raw _fullParagraph, clipped ≤700 chars
        }>;
    };
}

export interface ClusteringResult {
    clusters: ParagraphCluster[];
    meta: {
        totalClusters: number;
        singletonCount: number;
        uncertainCount: number;
        avgClusterSize: number;
        maxClusterSize: number;
        compressionRatio: number;    // clusters / paragraphs
        embeddingTimeMs: number;
        clusteringTimeMs: number;
        totalTimeMs: number;
    };
}

export interface EmbeddingResult {
    embeddings: Map<string, Float32Array>;  // paragraphId -> embedding
    dimensions: number;
    timeMs: number;
}
```

### File: `src/clustering/config.ts` (NEW)

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// CLUSTERING CONFIGURATION
// ═══════════════════════════════════════════════════════════════════════════

export interface ClusteringConfig {
    // Similarity threshold for merging (cosine similarity)
    // Higher = more clusters, stricter matching
    similarityThreshold: number;
    
    // ────────────────────────────────────────────────────────────────────────
    // v1.1: Max clusters safety limit
    // ────────────────────────────────────────────────────────────────────────
    maxClusters: number;
    
    // Uncertainty detection thresholds
    lowCohesionThreshold: number;
    maxClusterSize: number;
    stanceDiversityThreshold: number;      // Max unique stances before uncertain
    contestedRatioThreshold: number;       // Max ratio of contested paragraphs
    
    // Expansion limits
    maxExpansionMembers: number;
    maxExpansionCharsTotal: number;
    maxMemberTextChars: number;
    
    // Embedding configuration
    embeddingDimensions: number;
    modelId: string;
    
    // Minimum paragraphs to attempt clustering
    minParagraphsForClustering: number;
}

export const DEFAULT_CONFIG: ClusteringConfig = {
    // From traversal decision: 0.82 (moderately conservative)
    similarityThreshold: 0.82,
    
    // v1.1: Safety limit to prevent pathological explosion
    maxClusters: 40,
    
    // Uncertainty thresholds
    lowCohesionThreshold: 0.70,
    maxClusterSize: 8,
    stanceDiversityThreshold: 3,
    contestedRatioThreshold: 0.30,
    
    // Expansion limits
    maxExpansionMembers: 6,
    maxExpansionCharsTotal: 2100,
    maxMemberTextChars: 700,
    
    // Embedding
    embeddingDimensions: 256,
    modelId: 'all-MiniLM-L6-v2',
    
    // Minimum input
    minParagraphsForClustering: 3,
};

export const CONFIG_PRESETS = {
    highPrecision: {
        ...DEFAULT_CONFIG,
        similarityThreshold: 0.88,
        embeddingDimensions: 384,
    } as ClusteringConfig,
    
    balanced: DEFAULT_CONFIG,
    
    highRecall: {
        ...DEFAULT_CONFIG,
        similarityThreshold: 0.78,
    } as ClusteringConfig,
    
    fast: {
        ...DEFAULT_CONFIG,
        embeddingDimensions: 128,
    } as ClusteringConfig,
} as const;
```

### File: `src/clustering/distance.ts` (NEW)

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// DISTANCE & SIMILARITY CALCULATIONS
// ═══════════════════════════════════════════════════════════════════════════

/**
 * v1.1: Quantize similarity for deterministic comparisons.
 * Prevents floating-point drift across runs (GPU may vary slightly).
 */
export function quantizeSimilarity(sim: number): number {
    return Math.round(sim * 1e6) / 1e6;
}

/**
 * Cosine similarity between two normalized vectors.
 * Assumes vectors are already L2 normalized.
 */
export function cosineSimilarity(a: Float32Array, b: Float32Array): number {
    let dot = 0;
    const len = Math.min(a.length, b.length);
    for (let i = 0; i < len; i++) {
        dot += a[i] * b[i];
    }
    return dot;
}

/**
 * Build distance matrix from embeddings.
 * Returns distances (1 - similarity) for HAC algorithm.
 * 
 * v1.1: Uses quantized similarities for determinism.
 */
export function buildDistanceMatrix(
    ids: string[],
    embeddings: Map<string, Float32Array>
): number[][] {
    const n = ids.length;
    const distances: number[][] = Array(n).fill(null).map(() => Array(n).fill(0));
    
    for (let i = 0; i < n; i++) {
        const embA = embeddings.get(ids[i])!;
        for (let j = i + 1; j < n; j++) {
            const embB = embeddings.get(ids[j])!;
            const sim = cosineSimilarity(embA, embB);
            const simQ = quantizeSimilarity(sim);  // v1.1: Quantize
            const dist = 1 - simQ;
            distances[i][j] = dist;
            distances[j][i] = dist;
        }
    }
    
    return distances;
}

/**
 * Compute cluster cohesion (average similarity to centroid).
 * 
 * v1.1: Uses quantized similarities.
 */
export function computeCohesion(
    memberIds: string[],
    centroidId: string,
    embeddings: Map<string, Float32Array>
): number {
    if (memberIds.length <= 1) return 1.0;
    
    const centroidEmb = embeddings.get(centroidId)!;
    let totalSim = 0;
    
    for (const id of memberIds) {
        const emb = embeddings.get(id)!;
        const sim = cosineSimilarity(emb, centroidEmb);
        totalSim += quantizeSimilarity(sim);  // v1.1: Quantize
    }
    
    return totalSim / memberIds.length;
}
```

### File: `src/clustering/hac.ts` (NEW)

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// HIERARCHICAL AGGLOMERATIVE CLUSTERING
// ═══════════════════════════════════════════════════════════════════════════

import { ClusteringConfig } from './config';
import { quantizeSimilarity } from './distance';

/**
 * Average linkage: mean distance between all pairs across clusters.
 * This is generally best for semantic clustering.
 */
function averageLinkage(
    clusterA: Set<number>,
    clusterB: Set<number>,
    distances: number[][]
): number {
    let sum = 0;
    let count = 0;
    
    for (const i of clusterA) {
        for (const j of clusterB) {
            sum += distances[i][j];
            count++;
        }
    }
    
    return count > 0 ? sum / count : Infinity;
}

/**
 * Hierarchical Agglomerative Clustering with threshold-based stopping.
 * 
 * v1.1: Includes stable tie-breakers and max clusters safety limit.
 * 
 * Key behavior:
 * - Cluster count is EMERGENT from data, not forced
 * - Algorithm stops when nothing is similar enough to merge
 * - If cluster count exceeds maxClusters, forces merges to stay under limit
 * 
 * @param paragraphIds - Array of paragraph IDs in stable order
 * @param distances - Distance matrix (pre-computed, quantized)
 * @param config - Clustering configuration
 * @returns Array of clusters, each cluster is array of paragraph indices
 */
export function hierarchicalCluster(
    paragraphIds: string[],
    distances: number[][],
    config: ClusteringConfig
): number[][] {
    const n = paragraphIds.length;
    
    // Edge case: too few items
    if (n < config.minParagraphsForClustering) {
        return Array.from({ length: n }, (_, i) => [i]);
    }
    
    // ────────────────────────────────────────────────────────────────────────
    // Initialize: each item is its own cluster with stable IDs
    // ────────────────────────────────────────────────────────────────────────
    const clusters: Set<number>[] = Array.from({ length: n }, (_, i) => new Set([i]));
    const active = new Set(Array.from({ length: n }, (_, i) => i));
    
    // Convert similarity threshold to distance threshold
    const distanceThreshold = 1 - config.similarityThreshold;
    
    // ────────────────────────────────────────────────────────────────────────
    // Merge loop
    // ────────────────────────────────────────────────────────────────────────
    while (active.size > 1) {
        // Find closest pair with stable ordering for tie-breaking
        let minDist = Infinity;
        let minI = -1;
        let minJ = -1;
        
        // v1.1: Stable order for determinism
        const activeArray = Array.from(active).sort((a, b) => a - b);
        
        for (let ai = 0; ai < activeArray.length; ai++) {
            for (let aj = ai + 1; aj < activeArray.length; aj++) {
                const i = activeArray[ai];
                const j = activeArray[aj];
                const dist = quantizeSimilarity(averageLinkage(clusters[i], clusters[j], distances));
                
                // v1.1: Stable tie-breaker - prefer lower index pairs
                if (dist < minDist || (dist === minDist && (i < minI || (i === minI && j < minJ)))) {
                    minDist = dist;
                    minI = i;
                    minJ = j;
                }
            }
        }
        
        // ────────────────────────────────────────────────────────────────────
        // v1.1: Stop conditions with max clusters safety
        // ────────────────────────────────────────────────────────────────────
        
        // Under limit: stop if threshold exceeded
        if (active.size <= config.maxClusters && minDist > distanceThreshold) {
            break;
        }
        
        // Over limit: force merge even if over threshold (safety mechanism)
        // (Will continue loop until we're under limit or at 1 cluster)
        
        // ────────────────────────────────────────────────────────────────────
        // Merge j into i
        // ────────────────────────────────────────────────────────────────────
        for (const idx of clusters[minJ]) {
            clusters[minI].add(idx);
        }
        active.delete(minJ);
    }
    
    // ────────────────────────────────────────────────────────────────────────
    // Convert to stable array format
    // ────────────────────────────────────────────────────────────────────────
    return Array.from(active)
        .sort((a, b) => a - b)
        .map(i => Array.from(clusters[i]).sort((a, b) => a - b));
}
```

### File: `src/clustering/engine.ts` (NEW)

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// CLUSTERING ENGINE - MAIN ORCHESTRATION
// ═══════════════════════════════════════════════════════════════════════════

import { ShadowParagraph, ShadowStatement } from '../shadow';
import { ClusterableItem, ParagraphCluster, ClusteringResult } from './types';
import { ClusteringConfig, DEFAULT_CONFIG } from './config';
import { buildDistanceMatrix, cosineSimilarity, computeCohesion, quantizeSimilarity } from './distance';
import { hierarchicalCluster } from './hac';

// ═══════════════════════════════════════════════════════════════════════════
// HELPERS
// ═══════════════════════════════════════════════════════════════════════════

function clipText(text: string, maxChars: number): string {
    if (text.length <= maxChars) return text;
    return text.slice(0, maxChars).trim() + '...';
}

/**
 * Convert ShadowParagraph to ClusterableItem.
 * 
 * v1.1 CRITICAL: Build embedding text from original ShadowStatement texts,
 * NOT from _fullParagraph or prompt-clipped paragraph.statements[].text
 */
export function toClusterableItems(
    paragraphs: ShadowParagraph[],
    shadowStatements: ShadowStatement[]
): ClusterableItem[] {
    const statementsById = new Map(shadowStatements.map(s => [s.id, s]));
    
    return paragraphs.map(p => ({
        id: p.id,
        // v1.1: Build from unclipped statement texts in order
        text: p.statementIds
            .map(sid => statementsById.get(sid)?.text || '')
            .filter(t => t.length > 0)
            .join(' '),
        modelIndex: p.modelIndex,
        dominantStance: p.dominantStance,
        stanceHints: p.stanceHints,
        contested: p.contested,
        signals: p.signals,
        statementIds: p.statementIds,
    }));
}

/**
 * Find centroid: paragraph closest to cluster mean embedding.
 * 
 * v1.1: Includes stable tie-breaker for determinism.
 */
function findCentroid(
    memberIds: string[],
    embeddings: Map<string, Float32Array>
): { id: string; similarity: number } {
    if (memberIds.length === 1) {
        return { id: memberIds[0], similarity: 1.0 };
    }
    
    // ────────────────────────────────────────────────────────────────────────
    // Compute mean embedding
    // ────────────────────────────────────────────────────────────────────────
    const dim = embeddings.get(memberIds[0])!.length;
    const mean = new Float32Array(dim);
    
    for (const id of memberIds) {
        const emb = embeddings.get(id)!;
        for (let i = 0; i < dim; i++) {
            mean[i] += emb[i];
        }
    }
    
    // Normalize mean
    let norm = 0;
    for (let i = 0; i < dim; i++) {
        mean[i] /= memberIds.length;
        norm += mean[i] * mean[i];
    }
    norm = Math.sqrt(norm);
    if (norm > 0) {
        for (let i = 0; i < dim; i++) {
            mean[i] /= norm;
        }
    }
    
    // ────────────────────────────────────────────────────────────────────────
    // Find member closest to mean
    // v1.1: Stable tie-breaker by lexicographic ID
    // ────────────────────────────────────────────────────────────────────────
    let bestId = memberIds[0];
    let bestSim = -Infinity;
    
    for (const id of memberIds) {
        const emb = embeddings.get(id)!;
        let sim = 0;
        for (let i = 0; i < dim; i++) {
            sim += emb[i] * mean[i];
        }
        const simQ = quantizeSimilarity(sim);
        
        // Tie-breaker: lexicographically smallest ID
        if (simQ > bestSim || (simQ === bestSim && id < bestId)) {
            bestSim = simQ;
            bestId = id;
        }
    }
    
    return { id: bestId, similarity: bestSim };
}

/**
 * Detect uncertainty reasons for a cluster.
 * Checks multiple conditions in deterministic order.
 */
function detectUncertainty(
    paragraphIds: string[],
    paragraphsById: Map<string, ShadowParagraph>,
    cohesion: number,
    config: ClusteringConfig
): { uncertain: boolean; reasons: string[] } {
    const reasons: string[] = [];
    
    // Check in fixed order for determinism
    
    // 1. Low cohesion
    if (cohesion < config.lowCohesionThreshold) {
        reasons.push('low_cohesion');
    }
    
    // 2. Oversized
    if (paragraphIds.length > config.maxClusterSize) {
        reasons.push('oversized');
    }
    
    // 3. Stance diversity
    const uniqueStances = new Set<string>();
    for (const pid of paragraphIds) {
        const p = paragraphsById.get(pid);
        if (p) uniqueStances.add(p.dominantStance);
    }
    if (uniqueStances.size >= config.stanceDiversityThreshold) {
        reasons.push('stance_diversity');
    }
    
    // 4. Contested ratio
    let contestedCount = 0;
    for (const pid of paragraphIds) {
        const p = paragraphsById.get(pid);
        if (p?.contested) contestedCount++;
    }
    const contestedRatio = paragraphIds.length > 0 ? contestedCount / paragraphIds.length : 0;
    if (contestedRatio > config.contestedRatioThreshold) {
        reasons.push('high_contested_ratio');
    }
    
    // 5. Conflicting signals (tension + conditional both present)
    let hasTension = false;
    let hasConditional = false;
    for (const pid of paragraphIds) {
        const p = paragraphsById.get(pid);
        if (p?.signals.tension) hasTension = true;
        if (p?.signals.conditional) hasConditional = true;
    }
    if (hasTension && hasConditional && paragraphIds.length > 1) {
        reasons.push('conflicting_signals');
    }
    
    return {
        uncertain: reasons.length > 0,
        reasons,
    };
}

/**
 * Build expansion payload for uncertain clusters.
 * 
 * v1.1: Deterministic selection with stable ordering.
 * Uses _fullParagraph (raw text), not extracted statements.
 */
function buildExpansion(
    paragraphIds: string[],
    centroidId: string,
    paragraphsById: Map<string, ShadowParagraph>,
    embeddings: Map<string, Float32Array>,
    config: ClusteringConfig
): ParagraphCluster['expansion'] {
    const centroidEmb = embeddings.get(centroidId)!;
    
    // ────────────────────────────────────────────────────────────────────────
    // Find most distant members from centroid
    // ────────────────────────────────────────────────────────────────────────
    const memberSims = paragraphIds
        .map(pid => ({
            pid,
            sim: quantizeSimilarity(cosineSimilarity(embeddings.get(pid)!, centroidEmb))
        }))
        .sort((a, b) => a.sim - b.sim || a.pid.localeCompare(b.pid));  // Tie-break by ID
    
    // ────────────────────────────────────────────────────────────────────────
    // Select: centroid + most distant + fill to limit
    // ────────────────────────────────────────────────────────────────────────
    const selected = new Set<string>([centroidId]);
    for (const { pid } of memberSims) {
        if (selected.size >= config.maxExpansionMembers) break;
        selected.add(pid);
    }
    
    // ────────────────────────────────────────────────────────────────────────
    // Build expansion with char budget
    // ────────────────────────────────────────────────────────────────────────
    const members: Array<{ paragraphId: string; text: string }> = [];
    let charBudget = config.maxExpansionCharsTotal;
    
    // Centroid first, then by similarity order (distant first)
    for (const pid of [centroidId, ...memberSims.map(m => m.pid)]) {
        if (!selected.has(pid)) continue;
        selected.delete(pid);  // Don't add twice
        
        const p = paragraphsById.get(pid);
        if (!p) continue;
        
        // v1.1: Use raw _fullParagraph for expansion (not extracted statements)
        const text = clipText(p._fullParagraph || '', config.maxMemberTextChars);
        if (charBudget - text.length < 0) break;
        
        charBudget -= text.length;
        members.push({ paragraphId: pid, text });
    }
    
    return { members };
}

// ═══════════════════════════════════════════════════════════════════════════
// MAIN FUNCTION
// ═══════════════════════════════════════════════════════════════════════════

/**
 * Build paragraph clusters from paragraphs and embeddings.
 * 
 * v1.1: Accepts shadowStatements for building embedding text correctly.
 */
export function buildClusters(
    paragraphs: ShadowParagraph[],
    shadowStatements: ShadowStatement[],
    embeddings: Map<string, Float32Array>,
    config: ClusteringConfig = DEFAULT_CONFIG
): ClusteringResult {
    const startTime = performance.now();
    
    // Build lookup maps
    const paragraphsById = new Map(paragraphs.map(p => [p.id, p]));
    const paragraphIds = paragraphs.map(p => p.id);
    
    // ────────────────────────────────────────────────────────────────────────
    // Edge case: too few paragraphs
    // ────────────────────────────────────────────────────────────────────────
    if (paragraphs.length < config.minParagraphsForClustering) {
        const clusters: ParagraphCluster[] = paragraphs.map((p, idx) => ({
            id: `pc_${idx}`,
            paragraphIds: [p.id],
            statementIds: [...p.statementIds],
            representativeParagraphId: p.id,
            cohesion: 1.0,
            uncertain: false,
            uncertaintyReasons: [],
        }));
        
        return {
            clusters,
            meta: {
                totalClusters: clusters.length,
                singletonCount: clusters.length,
                uncertainCount: 0,
                avgClusterSize: 1,
                maxClusterSize: 1,
                compressionRatio: 1,
                embeddingTimeMs: 0,
                clusteringTimeMs: performance.now() - startTime,
                totalTimeMs: performance.now() - startTime,
            },
        };
    }
    
    // ────────────────────────────────────────────────────────────────────────
    // Build distance matrix
    // ────────────────────────────────────────────────────────────────────────
    const distances = buildDistanceMatrix(paragraphIds, embeddings);
    
    // ────────────────────────────────────────────────────────────────────────
    // Run HAC
    // ────────────────────────────────────────────────────────────────────────
    const clusterIndices = hierarchicalCluster(paragraphIds, distances, config);
    
    // ────────────────────────────────────────────────────────────────────────
    // Build ParagraphCluster objects
    // ────────────────────────────────────────────────────────────────────────
    const clusters: ParagraphCluster[] = [];
    let uncertainCount = 0;
    
    for (let i = 0; i < clusterIndices.length; i++) {
        const indices = clusterIndices[i];
        const memberIds = indices.map(idx => paragraphIds[idx]);
        
        // Find centroid
        const centroid = findCentroid(memberIds, embeddings);
        
        // Compute cohesion
        const cohesion = computeCohesion(memberIds, centroid.id, embeddings);
        
        // Detect uncertainty
        const uncertainty = detectUncertainty(memberIds, paragraphsById, cohesion, config);
        if (uncertainty.uncertain) uncertainCount++;
        
        // Collect statement IDs (stable order, no duplicates)
        const statementIds: string[] = [];
        const seenStatements = new Set<string>();
        for (const pid of memberIds) {
            const p = paragraphsById.get(pid);
            if (p) {
                for (const sid of p.statementIds) {
                    if (!seenStatements.has(sid)) {
                        seenStatements.add(sid);
                        statementIds.push(sid);
                    }
                }
            }
        }
        
        const cluster: ParagraphCluster = {
            id: `pc_${i}`,
            paragraphIds: memberIds,
            statementIds,
            representativeParagraphId: centroid.id,
            cohesion,
            uncertain: uncertainty.uncertain,
            uncertaintyReasons: uncertainty.reasons,
        };
        
        // Add expansion if uncertain
        if (uncertainty.uncertain) {
            cluster.expansion = buildExpansion(
                memberIds,
                centroid.id,
                paragraphsById,
                embeddings,
                config
            );
        }
        
        clusters.push(cluster);
    }
    
    // ────────────────────────────────────────────────────────────────────────
    // Sort: uncertain first, then by size descending
    // ────────────────────────────────────────────────────────────────────────
    clusters.sort((a, b) => {
        if (a.uncertain && !b.uncertain) return -1;
        if (!a.uncertain && b.uncertain) return 1;
        return b.paragraphIds.length - a.paragraphIds.length;
    });
    
    // Renumber IDs after sort for consistent ordering
    clusters.forEach((c, idx) => {
        c.id = `pc_${idx}`;
    });
    
    // ────────────────────────────────────────────────────────────────────────
    // Compute meta
    // ────────────────────────────────────────────────────────────────────────
    const clusteringTimeMs = performance.now() - startTime;
    const singletonCount = clusters.filter(c => c.paragraphIds.length === 1).length;
    const sizes = clusters.map(c => c.paragraphIds.length);
    
    return {
        clusters,
        meta: {
            totalClusters: clusters.length,
            singletonCount,
            uncertainCount,
            avgClusterSize: sizes.reduce((a, b) => a + b, 0) / sizes.length,
            maxClusterSize: Math.max(...sizes),
            compressionRatio: clusters.length / paragraphs.length,
            embeddingTimeMs: 0,  // Set by caller
            clusteringTimeMs,
            totalTimeMs: clusteringTimeMs,
        },
    };
}
```

### File: `src/clustering/embeddings.ts` (NEW)

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// EMBEDDING CLIENT - SERVICE WORKER SIDE
// ═══════════════════════════════════════════════════════════════════════════
//
// Communicates with offscreen document for embedding generation.
// 
// Key v1.1 changes:
// - Accepts shadowStatements to build embedding text from unclipped sources
// - Rehydrates Float32Array from JSON-serialized number[][]
// - Renormalizes embeddings after truncation for determinism
// ═══════════════════════════════════════════════════════════════════════════

import { ShadowParagraph, ShadowStatement } from '../shadow';
import { EmbeddingResult } from './types';
import { ClusteringConfig, DEFAULT_CONFIG } from './config';

/**
 * Ensure offscreen document exists for embedding inference.
 */
async function ensureOffscreen(): Promise<void> {
    const existingContexts = await chrome.runtime.getContexts({
        contextTypes: ['OFFSCREEN_DOCUMENT' as chrome.runtime.ContextType],
    });
    
    if (existingContexts.length > 0) return;
    
    await chrome.offscreen.createDocument({
        url: 'offscreen.html',
        reasons: [chrome.offscreen.Reason.WORKERS],
        justification: 'Embedding model inference for semantic clustering',
    });
}

/**
 * Normalize embedding vector (L2 norm).
 * v1.1: Critical for determinism after truncation.
 */
function normalizeEmbedding(vec: Float32Array): Float32Array {
    let norm = 0;
    for (let i = 0; i < vec.length; i++) {
        norm += vec[i] * vec[i];
    }
    norm = Math.sqrt(norm);
    
    if (norm > 0) {
        for (let i = 0; i < vec.length; i++) {
            vec[i] /= norm;
        }
    }
    
    return vec;
}

/**
 * Request embeddings from offscreen worker.
 * 
 * v1.1: Builds text from original ShadowStatement texts (unclipped),
 * rehydrates Float32Array, and renormalizes after truncation.
 */
export async function generateEmbeddings(
    paragraphs: ShadowParagraph[],
    shadowStatements: ShadowStatement[],
    config: ClusteringConfig = DEFAULT_CONFIG
): Promise<EmbeddingResult> {
    await ensureOffscreen();
    
    // ────────────────────────────────────────────────────────────────────────
    // v1.1: Build texts from original statement texts (NOT _fullParagraph)
    // ────────────────────────────────────────────────────────────────────────
    const statementsById = new Map(shadowStatements.map(s => [s.id, s]));
    const texts = paragraphs.map(p => 
        p.statementIds
            .map(sid => statementsById.get(sid)?.text || '')
            .filter(t => t.length > 0)
            .join(' ')
    );
    const ids = paragraphs.map(p => p.id);
    
    return new Promise((resolve, reject) => {
        chrome.runtime.sendMessage(
            {
                type: 'GENERATE_EMBEDDINGS',
                payload: {
                    texts,
                    dimensions: config.embeddingDimensions,
                },
            },
            (response) => {
                if (chrome.runtime.lastError) {
                    reject(new Error(chrome.runtime.lastError.message));
                    return;
                }
                
                if (!response?.success) {
                    reject(new Error(response?.error || 'Embedding generation failed'));
                    return;
                }
                
                // ────────────────────────────────────────────────────────────
                // v1.1: Rehydrate Float32Array and renormalize
                // ────────────────────────────────────────────────────────────
                const embeddings = new Map<string, Float32Array>();
                for (let i = 0; i < ids.length; i++) {
                    let emb = Float32Array.from(response.result.embeddings[i]);
                    
                    // Truncate if needed (MRL - Matryoshka Representation Learning)
                    if (emb.length > config.embeddingDimensions) {
                        emb = emb.slice(0, config.embeddingDimensions);
                    }
                    
                    // v1.1: Renormalize after truncation (critical for determinism)
                    emb = normalizeEmbedding(emb);
                    
                    embeddings.set(ids[i], emb);
                }
                
                resolve({
                    embeddings,
                    dimensions: config.embeddingDimensions,
                    timeMs: response.result.timeMs,
                });
            }
        );
    });
}

/**
 * Preload embedding model (call during idle time).
 */
export async function preloadModel(): Promise<void> {
    await ensureOffscreen();
    
    return new Promise((resolve, reject) => {
        chrome.runtime.sendMessage(
            { type: 'PRELOAD_MODEL' },
            (response) => {
                if (chrome.runtime.lastError) {
                    reject(new Error(chrome.runtime.lastError.message));
                } else if (response?.success) {
                    resolve();
                } else {
                    reject(new Error(response?.error || 'Model preload failed'));
                }
            }
        );
    });
}
```

### File: `src/clustering/index.ts` (NEW)

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// CLUSTERING MODULE - PUBLIC API
// ═══════════════════════════════════════════════════════════════════════════

// Types
export type {
    ClusterableItem,
    ParagraphCluster,
    ClusteringResult,
    EmbeddingResult,
} from './types';

// Configuration
export {
    type ClusteringConfig,
    DEFAULT_CONFIG,
    CONFIG_PRESETS,
} from './config';

// Main functions
export { generateEmbeddings, preloadModel } from './embeddings';
export { buildClusters, toClusterableItems } from './engine';

// Re-export for convenience
import { ShadowParagraph, ShadowStatement } from '../shadow';
import { ClusteringResult } from './types';
import { ClusteringConfig, DEFAULT_CONFIG } from './config';
import { generateEmbeddings } from './embeddings';
import { buildClusters } from './engine';

/**
 * High-level API: Cluster paragraphs end-to-end.
 * Handles embedding generation and clustering in one call.
 * 
 * v1.1: Requires shadowStatements for building embedding text correctly.
 */
export async function clusterParagraphs(
    paragraphs: ShadowParagraph[],
    shadowStatements: ShadowStatement[],
    config: Partial<ClusteringConfig> = {}
): Promise<ClusteringResult> {
    const mergedConfig: ClusteringConfig = { ...DEFAULT_CONFIG, ...config };
    
    // Skip if too few paragraphs
    if (paragraphs.length < mergedConfig.minParagraphsForClustering) {
        return buildClusters(paragraphs, shadowStatements, new Map(), mergedConfig);
    }
    
    // Generate embeddings
    const embeddingResult = await generateEmbeddings(paragraphs, shadowStatements, mergedConfig);
    
    // Build clusters
    const clusterResult = buildClusters(paragraphs, shadowStatements, embeddingResult.embeddings, mergedConfig);
    
    // Update timing
    clusterResult.meta.embeddingTimeMs = embeddingResult.timeMs;
    clusterResult.meta.totalTimeMs = embeddingResult.timeMs + clusterResult.meta.clusteringTimeMs;
    
    return clusterResult;
}


## Phase 4: Offscreen Embedding Worker

### Integration with Existing Bus System

Your codebase already has:
- **Offscreen document:** Created from `sw-entry.js`
- **Message bus:** `BusController.js` (JSON-based serialization)
- **Event handlers:** `OffscreenBootstrap.js`

**Key Gap:** Bus uses `JSON.stringify`/`JSON.parse`, which doesnt preserve `Float32Array`.

**Solution:**
- Offscreen returns `number[][]` over the bus
- Service worker rehydrates to `Float32Array[]`

### Message Contract

Register in offscreen (`OffscreenBootstrap.js` or new handler):


// embeddings.embedTexts
bus.register('embeddings.embedTexts', async (texts: string[], opts: { 
    dims: number; 
    modelId: string; 
    mode?: 'wasm' | 'webgpu' | 'auto' 
}) => {
    // Returns: { embeddings: number[][], dimensions: number, timeMs: number }
});

// embeddings.ping
bus.register('embeddings.ping', async () => {
    return { ready: !!embedder };
});

// embeddings.status
bus.register('embeddings.status', async () => {
    return { 
        ready: !!embedder, 
        backend: currentBackend,
        modelId: currentModelId 
    };
});
```

### File: `src/offscreen/embedding-worker.ts` (NEW)

// ═══════════════════════════════════════════════════════════════════════════
// EMBEDDING WORKER - OFFSCREEN DOCUMENT
// ═══════════════════════════════════════════════════════════════════════════
// Runs embedding model inference using WebGPU (primary) or WASM (fallback).
// Returns number[][] over the message bus (JSON-serializable).
//
// v1.1: Implements model caching and single-flight loading pattern.
// ═══════════════════════════════════════════════════════════════════════════

import { env, pipeline, Pipeline } from '@xenova/transformers';

// ═══════════════════════════════════════════════════════════════════════════
// CONFIGURATION
// ═══════════════════════════════════════════════════════════════════════════

// Use local models (bundled with extension)
env.allowRemoteModels = false;
env.localModelPath = chrome.runtime.getURL('models/');
env.backends.onnx.wasm.numThreads = 4;

// ═══════════════════════════════════════════════════════════════════════════
// STATE
// ═══════════════════════════════════════════════════════════════════════════

let modelCache: Map<string, Pipeline> = new Map();
let inFlightLoad: Promise<Pipeline> | null = null;
let currentBackend: 'webgpu' | 'wasm' | null = null;
let currentModelId: string | null = null;

// ═══════════════════════════════════════════════════════════════════════════
// MODEL INITIALIZATION
// ═══════════════════════════════════════════════════════════════════════════

async function ensureModel(
    modelId: string, 
    preferredBackend: 'webgpu' | 'wasm' | 'auto' = 'auto'
): Promise<Pipeline> {
    // Check cache first
    const hasWebGPU = !!navigator.gpu;
    const backend = preferredBackend === 'auto' 
        ? (hasWebGPU ? 'webgpu' : 'wasm')
        : preferredBackend;
    
    const cacheKey = `${modelId}:${backend}`;
    
    if (modelCache.has(cacheKey)) {
        return modelCache.get(cacheKey)!;
    }
    
    // Single-flight pattern: wait for any in-progress load
    if (inFlightLoad) {
        await inFlightLoad;
        if (modelCache.has(cacheKey)) {
            return modelCache.get(cacheKey)!;
        }
    }
    
    // Load model
    console.log(`[Embedder] Loading model ${modelId} with ${backend}...`);
    const startTime = performance.now();
    
    inFlightLoad = (async () => {
        let model: Pipeline;
        
        if (backend === 'webgpu' && hasWebGPU) {
            try {
                model = await pipeline('feature-extraction', modelId, {
                    device: 'webgpu',
                    dtype: 'q8',
                });
                currentBackend = 'webgpu';
                console.log(`[Embedder] Loaded with WebGPU in ${Math.round(performance.now() - startTime)}ms`);
            } catch (webgpuError) {
                console.warn('[Embedder] WebGPU failed, falling back to WASM:', webgpuError);
                model = await pipeline('feature-extraction', modelId, {
                    device: 'wasm',
                    dtype: 'q8',
                });
                currentBackend = 'wasm';
                console.log(`[Embedder] Loaded with WASM fallback in ${Math.round(performance.now() - startTime)}ms`);
            }
        } else {
            model = await pipeline('feature-extraction', modelId, {
                device: 'wasm',
                dtype: 'q8',
            });
            currentBackend = 'wasm';
            console.log(`[Embedder] Loaded with WASM in ${Math.round(performance.now() - startTime)}ms`);
        }
        
        currentModelId = modelId;
        modelCache.set(cacheKey, model);
        return model;
    })();
    
    const result = await inFlightLoad;
    inFlightLoad = null;
    return result;
}

// ═══════════════════════════════════════════════════════════════════════════
// EMBEDDING GENERATION
// ═══════════════════════════════════════════════════════════════════════════

async function generateEmbeddings(
    texts: string[],
    targetDimensions: number,
    modelId: string = 'all-MiniLM-L6-v2',
    mode: 'webgpu' | 'wasm' | 'auto' = 'auto'
): Promise<{ embeddings: number[][]; dimensions: number; timeMs: number }> {
    const embedder = await ensureModel(modelId, mode);
    
    const startTime = performance.now();
    const batchSize = 32;
    const allEmbeddings: number[][] = [];
    
    for (let i = 0; i < texts.length; i += batchSize) {
        const batch = texts.slice(i, i + batchSize);
        
        const outputs = await embedder(batch, {
            pooling: 'mean',
            normalize: true,
        });
        
        for (let j = 0; j < batch.length; j++) {
            const output = outputs[j];
            const data = output.data as Float32Array;
            
            // Truncate to target dimensions (MRL)
            const truncated = Array.from(data.slice(0, targetDimensions));
            
            // Note: Renormalization happens on service worker side
            // to keep offscreen worker simple
            
            allEmbeddings.push(truncated);
        }
    }
    
    return {
        embeddings: allEmbeddings,  // number[][] for JSON serialization
        dimensions: targetDimensions,
        timeMs: performance.now() - startTime,
    };
}

// ═══════════════════════════════════════════════════════════════════════════
// MESSAGE HANDLER
// ═══════════════════════════════════════════════════════════════════════════

chrome.runtime.onMessage.addListener((message, _sender, sendResponse) => {
    if (message.type === 'GENERATE_EMBEDDINGS') {
        const { texts, dimensions, modelId, mode } = message.payload;
        
        generateEmbeddings(texts, dimensions, modelId, mode)
            .then(result => sendResponse({ success: true, result }))
            .catch(error => {
                console.error('[Embedder] Generation failed:', error);
                sendResponse({ success: false, error: error.message });
            });
        
        return true;  // Async response
    }
    
    if (message.type === 'PRELOAD_MODEL') {
        const { modelId, mode } = message.payload || {};
        
        ensureModel(modelId || 'all-MiniLM-L6-v2', mode || 'auto')
            .then(() => sendResponse({ success: true }))
            .catch(error => sendResponse({ success: false, error: error.message }));
        
        return true;  // Async response
    }
    
    if (message.type === 'EMBEDDING_STATUS') {
        sendResponse({
            success: true,
            result: {
                ready: modelCache.size > 0,
                backend: currentBackend,
                modelId: currentModelId,
            },
        });
        return false;  // Sync response
    }
    
    return false;
});

// ═══════════════════════════════════════════════════════════════════════════
// PRELOAD ON DOCUMENT READY
// ═══════════════════════════════════════════════════════════════════════════

// Attempt background preload (non-blocking)
setTimeout(() => {
    ensureModel('all-MiniLM-L6-v2', 'auto').catch(err => {
        console.warn('[Embedder] Background preload failed:', err);
    });
}, 1000);
```

---

## Phase 5: Semantic Mapper Modifications

### File: `src/ConciergeService/semanticMapper.ts` (MODIFY)

**Key Changes:**
1. Remove inline `projectShadowParagraphs()` and `clusterParagraphs()` functions
2. Accept pre-computed projection and clustering as parameters
3. **Remove text duplication** - no `representativeText` in clusters
4. Fix prompt: remove metaphor, use domain-appropriate example
5. Implement uncertain-first cluster ordering

**New Signature:**

```typescript
import { ShadowStatement } from '../shadow';
import { ParagraphProjectionResult } from '../shadow/ShadowParagraphProjector';
import { ClusteringResult } from '../clustering';

export function buildSemanticMapperPrompt(
    userQuery: string,
    shadowStatements: ShadowStatement[],
    paragraphResult?: ParagraphProjectionResult,
    clusteringResult?: ClusteringResult | null
): string;
```

**Complete Implementation:**

```typescript
// ═══════════════════════════════════════════════════════════════════════════
// SEMANTIC MAPPER - PROMPT BUILDER
// ═══════════════════════════════════════════════════════════════════════════
//
// v1.1: Receives pre-computed paragraph projection and clustering.
// Prompt builder is SYNC - all async work done in StepExecutor.
//
// Critical invariants:
// - No text duplication (statements appear once)
// - Clusters contain IDs only (no representativeText)
// - Uncertain clusters listed first
// - Expansion uses _fullParagraph only
// ═══════════════════════════════════════════════════════════════════════════

import { ShadowStatement } from '../shadow';
import { ShadowParagraph, ParagraphProjectionResult } from '../shadow/ShadowParagraphProjector';
import { ParagraphCluster, ClusteringResult } from '../clustering';
import { SemanticMapperOutput } from './contract';
import { parseSemanticMapperOutput as baseParseOutput } from '../../shared/parsing-utils';

// ═══════════════════════════════════════════════════════════════════════════
// PROMPT BUILDER
// ═══════════════════════════════════════════════════════════════════════════

const MAX_CLUSTERS_IN_PROMPT = 15;

export function buildSemanticMapperPrompt(
    userQuery: string,
    shadowStatements: ShadowStatement[],
    paragraphResult?: ParagraphProjectionResult,
    clusteringResult?: ClusteringResult | null
): string {
    // ────────────────────────────────────────────────────────────────────────
    // Build paragraph block grouped by model
    // ────────────────────────────────────────────────────────────────────────
    let paragraphBlock = '{}';
    
    if (paragraphResult && paragraphResult.paragraphs.length > 0) {
        const groupedByModel: Record<string, any[]> = {};
        
        for (const p of paragraphResult.paragraphs) {
            const key = `model_${p.modelIndex}`;
            if (!groupedByModel[key]) groupedByModel[key] = [];
            
            // v1.1: Do NOT include paragraph.text - only statements[]
            groupedByModel[key].push({
                id: p.id,
                modelIndex: p.modelIndex,
                paragraphIndex: p.paragraphIndex,
                statementIds: p.statementIds,
                dominantStance: p.dominantStance,
                stanceHints: p.stanceHints,
                contested: p.contested,
                confidence: p.confidence,
                signals: p.signals,
                statements: p.statements,  // Contains text (single copy)
            });
        }
        
        paragraphBlock = JSON.stringify(groupedByModel);
    }
    
    // ────────────────────────────────────────────────────────────────────────
    // Build cluster block (uncertain first, capped)
    // ────────────────────────────────────────────────────────────────────────
    let clusterBlock = '';
    
    if (clusteringResult && clusteringResult.clusters.length > 0) {
        // Already sorted uncertain-first by engine, but verify and cap
        const sortedClusters = [...clusteringResult.clusters].sort((a, b) => {
            if (a.uncertain && !b.uncertain) return -1;
            if (!a.uncertain && b.uncertain) return 1;
            return b.paragraphIds.length - a.paragraphIds.length;
        });
        
        // Ensure all uncertain clusters are included, then fill to cap
        const uncertainClusters = sortedClusters.filter(c => c.uncertain);
        const certainClusters = sortedClusters.filter(c => !c.uncertain);
        const remainingSlots = Math.max(0, MAX_CLUSTERS_IN_PROMPT - uncertainClusters.length);
        const cappedClusters = [
            ...uncertainClusters,
            ...certainClusters.slice(0, remainingSlots)
        ];
        
        // v1.1: Do NOT include representativeText - only IDs
        const clusterData = cappedClusters.map(c => ({
            id: c.id,
            paragraphIds: c.paragraphIds,
            statementIds: c.statementIds,
            representativeParagraphId: c.representativeParagraphId,
            cohesion: c.cohesion,
            uncertain: c.uncertain,
            uncertaintyReasons: c.uncertaintyReasons,
            // expansion only for uncertain clusters (uses _fullParagraph text)
            ...(c.expansion ? { expansion: c.expansion } : {}),
        }));
        
        clusterBlock = JSON.stringify(clusterData);
    }
    
    // ────────────────────────────────────────────────────────────────────────
    // Build prompt (v1.1: cleaned up, no metaphor)
    // ────────────────────────────────────────────────────────────────────────
    return `You are a Semantic Cartographer. Your task is to organize extracted statements into claims with structural relationships.

<user_query>
"${userQuery}"
</user_query>

<shadow_paragraphs>
${paragraphBlock}
</shadow_paragraphs>

${clusterBlock ? `<paragraph_clusters>
${clusterBlock}
</paragraph_clusters>` : ''}

# Your Task

1. **Discover Claims**: Group statements pointing to the same position into a single claim. Name each claim with a short verb phrase.

2. **Trace Relationships**:
   - **Conditional gates**: Claims that only apply if some condition is true
   - **Prerequisite gates**: Claims whose validity depends on another claim
   - **Conflicts**: Claims that are mutually exclusive

3. For every gate or conflict, provide a **question** that would resolve it in concrete, human terms.

# Using the Input

- **shadow_paragraphs**: Contains statements grouped by model and paragraph. Each statement has an id (s_*), text, stance, and signals.
- **paragraph_clusters** (if present): Hints about which paragraphs express similar ideas. Use for grouping guidance.
  - If a cluster has \`uncertain: true\`, examine the \`expansion.members\` texts carefully and consider splitting into multiple claims.
  - If a paragraph has \`contested: true\`, it may contain multiple distinct positions - do not force into a single claim.
  - Clusters are HINTS only - you may split or ignore them based on semantic analysis.

# Schema Lock (Strict)

- Respond with a single JSON object only (no markdown, no prose).
- Do not output an "edges" field anywhere.
- The only relationship fields are "enables" and "conflicts".
- Every gate and every conflict must include a non-empty "question".
- **Never cite paragraph IDs (p_*) or cluster IDs (pc_*), only statement IDs (s_*).**

# Output Format

\`\`\`json
{
  "claims": [
    {
      "id": "c_0",
      "label": "make traversal the primary UI surface",
      "description": "The traversal/validity state should be the main view users interact with",
      "stance": "prescriptive",
      "gates": {
        "conditionals": [
          {
            "id": "cg_0",
            "condition": "semantic mapping produces non-narrative output",
            "question": "Is the semantic mapper output machine-legible rather than human-readable?",
            "sourceStatementIds": ["s_1", "s_39"]
          }
        ],
        "prerequisites": []
      },
      "enables": ["c_1"],
      "conflicts": [],
      "sourceStatementIds": ["s_1", "s_39", "s_40"]
    }
  ]
}
\`\`\`

# Field Names
- Claims: id, label, description?, stance, gates, enables, conflicts, sourceStatementIds
- Conditional gate: id, condition, question, sourceStatementIds
- Prerequisite gate: id, claimId, condition, question, sourceStatementIds
- Conflict: claimId, question, sourceStatementIds, nature?

# Provenance Requirements (Non-Negotiable)
- Every claim must include ≥1 sourceStatementId.
- Every gate must include ≥1 sourceStatementId.
- Every conflict must include ≥1 sourceStatementId.

Generate the map now.`;
}

// ═══════════════════════════════════════════════════════════════════════════
// PARSER WRAPPER
// ═══════════════════════════════════════════════════════════════════════════

export interface ParseResult {
    success: boolean;
    output?: SemanticMapperOutput;
    errors?: Array<{ field: string; issue: string; claimId?: string }>;
    warnings?: string[];
}

export function parseSemanticMapperOutput(
    rawResponse: string,
    shadowStatements: ShadowStatement[]
): ParseResult {
    const validIds = new Set(shadowStatements.map(s => s.id));
    const result = baseParseOutput(rawResponse, Array.from(validIds));

    // Type guard
    function isSemanticMapperOutput(parsed: unknown): parsed is SemanticMapperOutput {
        return (
            parsed !== null &&
            typeof parsed === 'object' &&
            'claims' in parsed &&
            Array.isArray((parsed as { claims: unknown }).claims)
        );
    }

    const output = isSemanticMapperOutput(result.output) ? result.output : undefined;
    const errors = result.errors ? [...result.errors] : [];
    
    if (result.success && !output) {
        errors.push({ field: 'output', issue: 'Invalid SemanticMapperOutput shape' });
    }

    return {
        success: result.success && !!output && errors.length === 0,
        output,
        errors,
        warnings: result.warnings
    };
}
```

---

## Phase 6: StepExecutor Integration

### File: `StepExecutor.js` (MODIFY)

**Location:** Inside `executeMappingStep()`, after shadow extraction.

**Replace this:**

```javascript
const shadowResult = extractShadowStatements(shadowInput);
const mappingPrompt = buildSemanticMapperPrompt(
    payload.originalPrompt,
    shadowResult.statements
);
```

**With this:**

```javascript
// ════════════════════════════════════════════════════════════════════════
// SHADOW EXTRACTION
// ════════════════════════════════════════════════════════════════════════
const shadowResult = extractShadowStatements(shadowInput);
console.log(`[StepExecutor] Extracted ${shadowResult.statements.length} shadow statements.`);

// ════════════════════════════════════════════════════════════════════════
// PARAGRAPH PROJECTION (sync, fast)
// ════════════════════════════════════════════════════════════════════════
const { projectParagraphs } = await import('../../shadow/ShadowParagraphProjector');
const paragraphResult = projectParagraphs(shadowResult.statements);
console.log(`[StepExecutor] Projected ${paragraphResult.paragraphs.length} paragraphs ` +
    `(${paragraphResult.meta.contestedCount} contested, ` +
    `${paragraphResult.meta.processingTimeMs.toFixed(1)}ms)`);

// ════════════════════════════════════════════════════════════════════════
// CLUSTERING (async, may fail gracefully)
// ════════════════════════════════════════════════════════════════════════
let clusteringResult = null;
if (paragraphResult.paragraphs.length >= 3) {
    try {
        const { clusterParagraphs } = await import('../../clustering');
        clusteringResult = await clusterParagraphs(
            paragraphResult.paragraphs,
            shadowResult.statements  // v1.1: Pass original statements for embedding text
        );
        
        console.log(`[StepExecutor] Clustered into ${clusteringResult.clusters.length} clusters ` +
            `(${clusteringResult.meta.singletonCount} singletons, ` +
            `${clusteringResult.meta.uncertainCount} uncertain, ` +
            `compression ${(clusteringResult.meta.compressionRatio * 100).toFixed(0)}%, ` +
            `embedding ${clusteringResult.meta.embeddingTimeMs.toFixed(0)}ms, ` +
            `clustering ${clusteringResult.meta.clusteringTimeMs.toFixed(0)}ms)`);
    } catch (clusteringError) {
        // Per design: skip clustering entirely on failure, continue without
        console.warn('[StepExecutor] Clustering failed, continuing without clusters:', clusteringError.message);
        clusteringResult = null;
    }
} else {
    console.log('[StepExecutor] Skipping clustering (< 3 paragraphs)');
}

// ════════════════════════════════════════════════════════════════════════
// BUILD PROMPT (sync, receives pre-computed data)
// ════════════════════════════════════════════════════════════════════════
const mappingPrompt = buildSemanticMapperPrompt(
    payload.originalPrompt,
    shadowResult.statements,
    paragraphResult,
    clusteringResult
);
```

---

## Phase 7: Build Configuration

### File: `package.json` (MODIFY)

Add dependency:

```json
{
    "dependencies": {
        "@xenova/transformers": "^2.17.0"
    }
}
```

### File: `scripts/postbuild.js` (MODIFY)

Add model copying:

```javascript
const fs = require('fs');
const path = require('path');

// ... existing postbuild logic ...

// ════════════════════════════════════════════════════════════════════════
// Copy embedding model artifacts
// ════════════════════════════════════════════════════════════════════════
const modelsSource = path.join(__dirname, '../models');
const modelsDest = path.join(__dirname, '../dist/models');

if (fs.existsSync(modelsSource)) {
    fs.cpSync(modelsSource, modelsDest, { recursive: true });
    console.log('[Postbuild] Copied models/ to dist/models/');
} else {
    console.warn('[Postbuild] Warning: models/ directory not found');
}
```

### File: `manifest.json` (VERIFY/MODIFY)

Ensure these exist:

```json
{
    "permissions": [
        "offscreen"
    ],
    "web_accessible_resources": [{
        "resources": ["models/*"],
        "matches": ["<all_urls>"]
    }]
}
```

### Model Files

Create `models/` directory and download ONNX files:

```
models/
└── all-MiniLM-L6-v2/
    ├── config.json
    ├── tokenizer.json
    ├── tokenizer_config.json
    └── onnx/
        └── model_quantized.onnx
```

Download from: https://huggingface.co/Xenova/all-MiniLM-L6-v2/tree/main

---

## Phase 8: Verification Checklist

### Acceptance Criteria

| # | Criterion | How to Verify |
|---|-----------|---------------|
| 1 | No text duplication | Grep prompt for repeated statement text |
| 2 | Provenance enforced | Parse fails on any `p_*` / `pc_*` citations |
| 3 | Determinism | Run same input 3x, compare cluster IDs/membership |
| 4 | Contested surfaced | Paragraphs with contradiction pairs have `contested: true` |
| 5 | Centroid not first | Verify representative isnt always first paragraph |
| 6 | Uncertain-first ordering | Uncertain clusters appear first in prompt |
| 7 | Embedding failure graceful | Mock failure, verify mapping continues |
| 8 | Cross-model clustering | UI design query produces clusters spanning models |
| 9 | Markdown filtered | No `##` headers in statement output |
| 10 | Tables skipped | No `|` fragments in statement output |
| 11 | Max clusters enforced | Never exceed 40 clusters regardless of input |
| 12 | Quantized similarities | Same input always produces same similarity values |

### Unit Tests to Create

| Test File | What to Test |
|-----------|--------------|
| `tests/shadow/ShadowExtractor.test.ts` | Markdown header filtering, table skipping |
| `tests/shadow/ShadowParagraphProjector.test.ts` | Projection, contested detection, stance aggregation |
| `tests/clustering/distance.test.ts` | Cosine similarity, quantization |
| `tests/clustering/hac.test.ts` | Determinism, tie-breakers, max clusters |
| `tests/clustering/engine.test.ts` | Centroid selection, uncertainty detection |
| `tests/parsing/provenance.test.ts` | Reject `p_*` / `pc_*` / unknown IDs |

### Regression Test Case

Save the failing UI design query prompt as a regression test:
- Input: Two models with identical 3-zone UI descriptions using different vocabulary
- Expected: Cross-model clusters form (e.g., "Logic Foundry" ≈ "Traversal Dashboard")
- Validate: Singleton rate < 50%, at least 3 cross-model clusters

---

## Implementation Order

```
Phase 0 (Day 1): Parser Enforcement
├── Add validateProvenance() to shared/parsing-utils.ts
├── Wire into semanticMapper parsing flow
└── Unit tests for provenance rejection

Phase 1 (Day 2): Shadow Fixes + Projector
├── Modify ShadowExtractor (markdown, tables)
├── Create ShadowParagraphProjector.ts
├── Export from shadow/index.ts
└── Unit tests

Phase 2 (Days 3-4): Embeddings
├── Add @xenova/transformers dependency
├── Create embedding-worker.ts in offscreen
├── Create embeddings.ts service wrapper
├── Add model bundling to postbuild.js
├── Verify manifest.json resources
└── Integration tests

Phase 3 (Days 5-7): Clustering
├── types.ts (with maxClusters)
├── config.ts (with maxClusters: 40)
├── distance.ts (with quantizeSimilarity)
├── hac.ts (with tie-breakers, max clusters)
├── engine.ts (accepts shadowStatements)
├── index.ts
└── Unit tests

Phase 4 (Days 8-9): Integration
├── StepExecutor changes
├── semanticMapper changes (remove inline code, accept params)
├── Remove representativeText from cluster output
└── End-to-end tests

Phase 5 (Days 10-11): Polish
├── Edge case handling
├── Performance tuning
├── Regression test with UI design query
└── Documentation
```

---

## Summary of What's Implemented

### New Files (10)
```
src/shadow/ShadowParagraphProjector.ts    ✓ Complete
src/clustering/types.ts                    ✓ Complete
src/clustering/config.ts                   ✓ Complete
src/clustering/distance.ts                 ✓ Complete
src/clustering/hac.ts                      ✓ Complete
src/clustering/engine.ts                   ✓ Complete
src/clustering/embeddings.ts               ✓ Complete
src/clustering/index.ts                    ✓ Complete
src/offscreen/embedding-worker.ts          ✓ Complete
models/all-MiniLM-L6-v2/                   → Download from HuggingFace
```

### Modified Files (7)
```
src/shadow/ShadowExtractor.ts              ✓ Complete (markdown/table filters)
src/shadow/index.ts                        ✓ Complete (exports)
src/ConciergeService/semanticMapper.ts     ✓ Complete (new signature, no duplication)
StepExecutor.js                            ✓ Complete (projection + clustering)
shared/parsing-utils.ts                    ✓ Complete (provenance validation)
manifest.json                              ✓ Complete (resources)
package.json                               ✓ Complete (dependency)
scripts/postbuild.js                       ✓ Complete (model copying)
```

### v1.1 Critical Corrections Applied
1. ✓ Quantized similarity for determinism
2. ✓ Stable tie-breakers in HAC
3. ✓ Max clusters safety limit (40)
4. ✓ Embedding input from unclipped statement texts
5. ✓ Parser enforcement for provenance
6. ✓ No representativeText duplication

---

**This plan is now complete and ready for implementation.**